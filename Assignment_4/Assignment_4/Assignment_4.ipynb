{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE-6524 / CS-6524 Deep Learning\n",
    "# Assignment 4 [100 pts]\n",
    "\n",
    "In this assignment, we will explore Recurrent Neural Networks (RNN) to deal with the data with temporal sequence. Specifically, we will generate captions for images. We will design an encoder-decoder architecture to achieve this. This homework is inspired by Stanford CS231n and UCSD CSE253.\n",
    "\n",
    "## Submission guideline for the coding part (Jupyter Notebook)\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook\n",
    "2. Please make sure to have entered your Virginia Tech PID below\n",
    "3. Once you've completed everything (make sure output for all cells are visible), select File -> Download as -> PDF via LaTeX\n",
    "4. Look at the PDF file and make sure all your solutions are displayed correctly there \n",
    "7. Zip all the files along with this notebook (Please don't include the data). Name it as Assignment_3_Code_[YOUR PID NUMBER].zip\n",
    "8. Name your PDF file as Assignment_4_NB_[YOUR PID NUMBER].pdf\n",
    "9. **<span style=\"color:blue\"> Submit your zipped file and the PDF SEPARATELY**</span>\n",
    "\n",
    "Note: if facing issues with step 3 refer: https://pypi.org/project/notebook-as-pdf/\n",
    "\n",
    "## Submission guideline for the coding part (Google Colab)\n",
    "\n",
    "1. Click the Save button at the top of the Notebook\n",
    "2. Please make sure to have entered your Virginia Tech PID below\n",
    "3. Follow last two cells in this notebook for guidelines to download pdf file of this notebook\n",
    "4. Look at the PDF file and make sure all your solutions are displayed correctly there \n",
    "5. Zip all the files along with this notebook (Please don't include the data). Name it as Assignment_2_Code_[YOUR PID NUMBER].zip\n",
    "6. Name your PDF file as Assignment_4_NB_[YOUR PID NUMBER].pdf\n",
    "7. **<span style=\"color:blue\"> Submit your zipped file and the PDF SEPARATELY**</span>\n",
    "\n",
    "**While you are encouraged to discuss with your peers, <span style=\"color:blue\">all work submitted is expected to be your own.</span> <span style=\"color:red\">If you use any information from other resources (e.g. online materials), you are required to cite it below you VT PID. Any violation will result in a 0 mark for the assignment.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please Write Your VT PID Here: \n",
    "### Reference (if any):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you would need to use **Python 3.6+** along with the following packages (**need to update**):\n",
    "```\n",
    "1. pytorch 1.2\n",
    "2. torchvision\n",
    "3. numpy\n",
    "4. matplotlib\n",
    "5. nltk\n",
    "```\n",
    "To install pytorch, please follow the instructions on the [Official website](https://pytorch.org/). In addition, the [official document](https://pytorch.org/docs/stable/) could be very helpful when you want to find certain functionalities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages and modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning Using Encoder-Decoder Architecture\n",
    "\n",
    "Simply, the encoder will take the image as input and encode it into a vector of feature values. The decoder will take this output from encoder as hidden state and starts to predict next words at each step. The following figure illustrates this:\n",
    "\n",
    "<img src=\"figs/image_captioning_overview.jpg\" width=\"600\">\n",
    "Figure 1. An overview of the encoder-decoder architecture\n",
    "(image credit: <a href=\"https://link.springer.com/chapter/10.1007/978-3-030-04780-1_23\">Deep Neural Network Based Image Captioning</a>)\n",
    "\n",
    "You will use a pre-trained CNN as the encoder and Vanilla RNN/LSTM as decoder to predict the captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1.1 Data\n",
    "Download dataset following the instructions. We are gonna use Flickr30k dataset, which consists of 31783 images and 158,915 captions. However, instead of using the whole dataset and \"results.csv\", we will follow Karpathy's Flickr30k annotations and use \"dataset_flickr30k.json\". Please move \"dataset_flickr30k.json\" to \"flickr30k_images\" folder.\n",
    "\n",
    "## How to download the data (ARC)\n",
    "Since the original Flickr30K dataset requires application, we will use the dataset from Kaggle. \n",
    "\n",
    "Step 1: Register a Kaggle account. https://www.kaggle.com/\n",
    "\n",
    "Step 2: Log into ARC server in the terminal, e.g. Huckleberry.\n",
    "\n",
    "Step 3: Install required packages.\n",
    "\n",
    "    - if you're gonna use powerai on Huckleberry\n",
    "        # step 1: request for GPU nodes\n",
    "        salloc --partition=normal_q --nodes=1 --tasks-per-node=10 --gres=gpu:1 bash\n",
    "        # or if you don't want a GPU, it will be faster to get a job\n",
    "        salloc --partition=normal_q --nodes=1 --tasks-per-node=10 bash\n",
    "        # step 2: load all necessary modules\n",
    "        module load gcc cuda Anaconda3 jdk\n",
    "        # step 3: activate the virtual environment\n",
    "        source activate powerai16_ibm\n",
    "        # step 4: for new packages(take tqdm for example)\n",
    "        pip install --user kaggle nltk # on hulogin1/hulogin2\n",
    "        \n",
    "    - if you're gonna use your own conda environment, simply type\n",
    "        pip install kaggle nltk\n",
    "Step 4: Make sure you have kaggle. Type `kaggle`.\n",
    "\n",
    "Step 5: Download your kaggle.json file from  https://www.kaggle.com/Your_Username/account. In API section, click Create New API Token. Then move you kaggle.json file to the path `/home/your_name_space/.kaggle/kaggle.json`.\n",
    "\n",
    "Step 6: Download the dataset. Type `kaggle datasets download hsankesara/flickr-image-dataset`.\n",
    "\n",
    "Step 7: Unzip the dataset. Type `unzip flickr-image-dataset.zip -x \"flickr30k_images/flickr30k_images/flickr30k_images/*.jpg\" -d \"/path-to-Assignment_4/Assignment_4/\"`\n",
    "\n",
    "Step 8: You should have your dataset in `/path-to-Assignment_4/Assignment_4/flickr30k_images/`\n",
    "\n",
    "Step 9: Move \"dataset_flickr30k.json\" to \"flickr30k_images\" folder.\n",
    "\n",
    "**Note** that you might want to use `nltk.download('punkt')` and `torchvision.models.resnet50(pretrained=True)` before you enter the GPU node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to download the data (Google Colab)\n",
    "Step 1: Register a Kaggle account.  https://www.kaggle.com/\n",
    "\n",
    "Step 2: Download your kaggle.json file from  https://www.kaggle.com/Your_Username/account. In API section, click Create New API Token.\n",
    "\n",
    "Step 3: As we did before, upload all files on Google Drive and open Google Colab.\n",
    "\n",
    "Step 4: Install required packages.\n",
    "    \n",
    "    ! pip install -q kaggle nltk\n",
    "\n",
    "Step 5: Insert a cell.\n",
    "    \n",
    "    \n",
    "    from google.colab import files\n",
    "    files.upload()\n",
    "    \n",
    "    \n",
    "    Upload `kaggle.json` you just downloaded.\n",
    "    \n",
    "Step 6: Move `kaggle.json` to the right place,\n",
    "    \n",
    "    \n",
    "     ! mkdir ~/.kaggle\n",
    "     ! cp kaggle.json ~/.kaggle/\n",
    "    \n",
    "\n",
    "Step 7: Change the permission.\n",
    "    \n",
    "    ! chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "Step 8: Download.\n",
    "    \n",
    "    !kaggle datasets download hsankesara/flickr-image-dataset\n",
    "\n",
    "Step 9: Move it to your drive and unzip it.\n",
    "    \n",
    "    unzip flickr-image-dataset.zip -x \"flickr30k_images/flickr30k_images/flickr30k_images/*.jpg\" -d \"/path-to-Assignment_4/Assignment_4/\"\n",
    "    \n",
    "Step 10: Move \"dataset_flickr30k.json\" to \"flickr30k_images\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab Setup: \n",
    "- Below are some basic steps for colab setup. \n",
    "- Make changes based on requirements.\n",
    "- Comment out in case of ARC or your local device with powerful GPU.\n",
    "\n",
    "**Note: For Google Colab give proper paths in this notebook and in dataloader.py if required.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# modify \"customized_path_to_homework\", path of folder in drive, where you uploaded your homework\n",
    "path_to_homework = \"/content/drive/My Drive/DL/Assignment_4/\"\n",
    "sys.path.append(path_to_homework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import Flickr30k, get_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1.2 Take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize images and captions\n",
    "flickr = Flickr30k(split='val', root=path_to_homework+'flickr30k_images/')  # load validation set as an example\n",
    "flickr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a random image and its captions\n",
    "img_id = np.random.randint(len(flickr))\n",
    "img = flickr.get_img(img_id)\n",
    "captions = flickr.get_captions(img_id)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "\n",
    "print(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del flickr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1.3 Build vocabulary\n",
    "We need to build a vocabulary for our dataset. The vocabulary stores all the words and their indices. We will use it to embed and recover the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "nltk.download('punkt') # You can comment this line once you've downloaded 'punkt'\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {'<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3}  # follow Pytorch padding rules: pad sentence with zero.\n",
    "        self.idx = 4\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "\n",
    "    def __call__(self, key):\n",
    "        if key not in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[key]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"\n",
    "        Add new words\n",
    "        :param word: word\n",
    "        \"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx  # add a new word\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def reverse(self, value):\n",
    "        \"\"\"\n",
    "        From idx to words.\n",
    "        :param value: index\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if value not in self.idx2word:\n",
    "            return self.idx2word[1]  # return '<unk>' if the word is unseen before.\n",
    "        return self.idx2word[value]\n",
    "\n",
    "def build_vocab(json_file=path_to_homework+ '/flickr30k_images/dataset_flickr30k.json', threshold=3):\n",
    "    with open(json_file) as f:\n",
    "            data = json.load(f)\n",
    "    f.close()\n",
    "    counter = Counter()\n",
    "    for img_idx in tqdm(range(len(data['images']))):\n",
    "        img_annos = data['images'][img_idx]\n",
    "        for sent_idx in range(len(img_annos['sentids'])):\n",
    "#             tokens = img_annos['sentences'][sent_idx]['tokens']  # directly load tokens\n",
    "\n",
    "            caption = img_annos['sentences'][sent_idx]['raw']\n",
    "            tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "            \n",
    "            counter.update(tokens)\n",
    "\n",
    "    # If the number of words is less than threshold we don't count it.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "\n",
    "    # create a Vocabulary class\n",
    "    vocab = Vocabulary()\n",
    "\n",
    "    # add words to Vocab\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a vocabulary for future usage\n",
    "vocab_path = path_to_homework + '/flickr30k_images/vocab.pkl'\n",
    "if not os.path.isfile(vocab_path):  # if we don't have vocab, create one\n",
    "    vocab = build_vocab(json_file=path_to_homework + '/flickr30k_images/dataset_flickr30k.json', threshold=3)\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
    "    print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))\n",
    "else:  # if we have, load the existing vocab\n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    print('vocab loaded!')\n",
    "    print('the size of vocab:', len(vocab))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = path_to_homework + '/flickr30k_images/vocab.pkl'\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "print('vocab loaded!')\n",
    "print('the size of vocab:', len(vocab))\n",
    "# print(vocab.word2idx.keys())\n",
    "# print(vocab.idx2word)\n",
    "\n",
    "# check some random words\n",
    "for i in range(3):\n",
    "    random_idx = np.random.randint(len(vocab))\n",
    "    print('word: {}, index: {}'.format(list(vocab.word2idx.keys())[random_idx], vocab(list(vocab.word2idx.keys())[random_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 Vanilla RNN [45 pts]\n",
    "# Section 2.1 Design the Network: Encoder [5 pts]\n",
    "Implement the baseline model by using pre-trained ResNet-50 as the encoder and Vanilla RNN as the decoder. Note that we will remove the last layer (fc layer) of ResNet-50 and add a trainable linear layer to finetune it for our task. During the training, we will **freeze** the layer before the fc layer. The encoder should output a feature vector of a fixed size for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        \"\"\"\n",
    "        Use ResNet-50 as encoder.\n",
    "        :param emb_dim: output size of ResNet-50.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.resnet = torchvision.models.resnet50(pretrained=True)\n",
    "        ###########Your code###############\n",
    "        # freeze the parameters\n",
    "        \n",
    "        # replace the last layer (fc layer) with a trainable layer for finetuning\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)  # output shape: [N, emb_dim]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2.2 Design the Network: Decoder [10 pts]\n",
    "During decoding, we will train a RNN (https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN) to learn the structure of the caption text throught \"**Teacher Forcing**\". Teacher forcing works by using the teaching signal from the training dataset at the current time step, $target(t)$, as input in the next time step $x(t+1) = target(t)$, rather than the output $y(t)$ generated by the network. \n",
    "\n",
    "As shown in Figure 1 above, RNN will take three inputs: the *current feature*, hidden state ($h_0$) and cell state ($c_0$). The *current feature* for the first step should be the output of encoder to predict '\\<start\\>' word. Hidden states for this step should be set to None. Then in the second step '\\<start\\>' will be passed into RNN as the input, and so on.\n",
    "\n",
    "To use '\\<start\\>' or any subsequent word as current feature, get its index from the vocabulary you created, convert it to one-hot vector and pass it through a linear layer to embed into a feature (or you can take advantage of Pytorch’s nn.Embedding which does one-hot encoding + linear layer for you).\n",
    "\n",
    "For convenience, you might want to 'pad' the captions in a mini-batch to convert them into fixed length. You can use 'pack_padded_sequence' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers=1, dropout=0):\n",
    "        \"\"\"\n",
    "        Use RNN as decoder for captions.\n",
    "        :param emb_dim: Embedding dimensions.\n",
    "        :param hidden_dim: Hidden states dimensions.\n",
    "        :param num_layers: Number of RNN layers.\n",
    "        :param vocab_size: The size of Vocabulary.\n",
    "        :param dropout: the probability for dropout.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.max_length = 30  # the maximum length of a sentence, in case it's trapped\n",
    "        \n",
    "        #############Your code############\n",
    "        # you need to implement a Vanilla RNN for the decoder. Take a look at the official documentation.\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN\n",
    "        \n",
    "        # one-hot encoding + linear layer\n",
    "        \n",
    "        # vanilla rnn network\n",
    "        \n",
    "        # output layer\n",
    "        \n",
    "\n",
    "    def forward(self, encode_features, captions, lengths):\n",
    "        \"\"\"\n",
    "        Feed forward to generate captions. Note that you need to pad the input so they have the same length\n",
    "        :param encode_features: output of encoder, size [N, emb_dim]\n",
    "        :param captions: captions, size [N, max(lengths)]\n",
    "        :param lengths: a list indicating valid length for each caption. size is (batch_size).\n",
    "        \"\"\"\n",
    "        #############Your Code###################\n",
    "        # compute the embedding using one-hot technique and linear function\n",
    "        \n",
    "        # concatenate the encoded features from encoder and embeddings\n",
    "                \n",
    "        # feed into RNN.\n",
    "        \n",
    "        # output layer\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-decoder [10 pts]\n",
    "Now we need to put our encoder and decoder together. \n",
    "\n",
    "In the sample_generate stage, the idea is to “let the network run on its own”, predicting the next word, and then use the network’s prediction to obtain the next input word. There are at least two ways to obtain the next word.\n",
    "\n",
    "- **Deterministic**: Take the maximum output at each step.\n",
    "- **Stochastic**: Sample from the probability distribution. To get the distribution, we need to compute the weighted softmax of the outputs: $y^i = \\exp(o^j/\\tau) / \\sum_n \\exp(o^n/\\tau)$, where $o^j$ is the output from the last layer, $n$ is the size of the vocabulary, and $\\tau$ is the so-called \"temperature\". By doing this, you should get a different caption each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla_rnn(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers=1, dropout=0):\n",
    "        \"\"\"\n",
    "        Encoder-decoder vanilla RNN.\n",
    "        :param vocab_size: the size of Vocabulary.\n",
    "        :param emb_dim: the dimensions of word embedding.\n",
    "        :param hidden_dim: the dimensions of hidden units.\n",
    "        :param num_layers: the number of RNN layers.\n",
    "        :param dropout: dropout probability\n",
    "        \"\"\"\n",
    "        super(Vanilla_rnn, self).__init__()\n",
    "        self.max_length = self.decoder.max_length\n",
    "        #########Your Code################\n",
    "        # Encoder: ResNet-50\n",
    "\n",
    "        # Decoder: RNN\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x, captions, lengths):\n",
    "        \"\"\"\n",
    "        Feed forward.\n",
    "        :param x: Images, [N, 3, H, W]\n",
    "        :param captions: encoded captions, [N, max(lengths)]\n",
    "        :param lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "        :return: output logits, usually followed by a softmax layer.\n",
    "        \"\"\"\n",
    "        ##########Your code###################\n",
    "        # forward passing\n",
    "\n",
    "        return x\n",
    "\n",
    "    def sample_generate(self, x, states=None, mode='Deterministic', temperature=5.0):\n",
    "        \"\"\"\n",
    "        Generate samples during the evaluation.\n",
    "        \n",
    "        :param x: input image\n",
    "        :param states: rnn states\n",
    "        :param mode: which mode we use.  \n",
    "         - 'Deterministic': Take the maximum output at each step.\n",
    "         - 'Stochastic': Sample from the probability distribution from the output layer.\n",
    "        :param temperature: will be used in the stochastic mode\n",
    "        :return: sample_idxs. Word indices. We can use vocab to recover the sentence later.\n",
    "        \"\"\"\n",
    "        sample_idxs = []  # record the index of your generated words\n",
    "        #################Your Code##################\n",
    "        # compute the encoded features\n",
    "        \n",
    "        # decide which mode we use\n",
    "        if mode == 'Deterministic':\n",
    "            # take the maximum index after the softmax\n",
    "            \n",
    "        elif mode == 'Stochastic':\n",
    "            # sample from the probability distribution after the softmax\n",
    "            # Hint: use torch.multinomial() to sample from a distribution.\n",
    "            \n",
    "        return sample_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2.3 Training [10 pts]\n",
    "Train your encoder-decoder. You might also want to check the output sentence every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some hyperparameters, you can change them\n",
    "## training parameters\n",
    "batch_size = 256\n",
    "lr = 1e-2\n",
    "num_epochs = 50\n",
    "weight_decay = 0.0\n",
    "log_step = 50\n",
    "\n",
    "## network architecture\n",
    "emb_dim = 1024\n",
    "hidden_dim = 256\n",
    "num_layers = 1 # number of RNN layers\n",
    "dropout = 0.0\n",
    "\n",
    "## image transformation\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        #     transforms.RandomCrop(224, pad_if_needed=True),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                             std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "## Output directory\n",
    "output_dir = path_to_homework + '/checkpoints/rnn/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "## device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation code here. We are gonna use this during the training. \n",
    "def val(model, data_loader, vocab):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    :param model: the encoder-decoder network.\n",
    "    :param data_loader: validation data loader\n",
    "    :param vocab: pre-built vocabulary\n",
    "    Output:\n",
    "    the mean value of validation losses\n",
    "    \"\"\"\n",
    "    print('Validating...')\n",
    "    val_loss = []\n",
    "    total_step = len(data_loader)\n",
    "    for itr, (images, captions, lengths) in enumerate(data_loader):\n",
    "        #######Your Code#########\n",
    "        # forward inputs and compute the validation loss\n",
    "        \n",
    "        # record the validation loss\n",
    "        \n",
    "        # Print current loss\n",
    "        if itr % log_step == 0:\n",
    "            print('Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                  .format(itr, total_step, loss.item(), np.exp(loss.item())))\n",
    "    \n",
    "    # (optional) you might also want to print out the sentence to see the qualitative performance of your model. \n",
    "    # You can use deterministic mode to generate sentences\n",
    "    \n",
    "\n",
    "    return np.mean(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training code here\n",
    "\n",
    "\n",
    "train_data_loader = get_loader(root=path_to_homework + '/flickr30k_images/', split='train', vocab=vocab, \n",
    "                               transform=transform, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_data_loader = get_loader(root=path_to_homework + '/flickr30k_images/', split='val', vocab=vocab, \n",
    "                               transform=transform, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "model = Vanilla_rnn(vocab_size=len(vocab), emb_dim=emb_dim, hidden_dim=hidden_dim, \n",
    "                   num_layers=1, dropout=dropout).to(device)  # build a model\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # CE loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)  # optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "                                      step_size=5,\n",
    "                                      gamma=0.5)  # decay LR by a factor of 0.5 every 10 epochs. You can change this\n",
    "\n",
    "# logs\n",
    "Train_Losses = []  # record average training loss each epoch\n",
    "Val_Losses = []   # record average validation loss each epoch\n",
    "total_step = len(train_data_loader)  # number of iterations each epoch\n",
    "best_val_loss = np.inf\n",
    "\n",
    "# start training\n",
    "print('Start training...')\n",
    "import time\n",
    "tic = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    print('Switch to training...')\n",
    "    model.train()\n",
    "    Train_loss_iter = []  # record the the training loss each iteration\n",
    "    for itr, (images, captions, lengths) in enumerate(train_data_loader):\n",
    "        ########Your Code###########\n",
    "        # train your model\n",
    "        \n",
    "        # record the training loss\n",
    "        \n",
    "        \n",
    "        # print log info\n",
    "        if itr % log_step == 0:\n",
    "            # print current loss and perplexity\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                      .format(epoch, num_epochs, itr, total_step, loss.item(), np.exp(loss.item())))\n",
    "    scheduler.step()\n",
    "    Train_Losses.append(np.mean(Train_loss_iter))\n",
    "    np.save(os.path.join(output_dir, 'TrainingLoss_rnn.npy'), Train_Losses)  # save the training loss\n",
    "    \n",
    "    model.eval()\n",
    "    # (optional) generate a sample during the training, you can use deterministic mode\n",
    "    # Your code\n",
    "    \n",
    "    \n",
    "    # validation\n",
    "    Val_Losses.append(val(model, val_data_loader, vocab))\n",
    "    np.save(os.path.join(output_dir, 'ValLoss_rnn.npy'), Val_Losses) # save the val loss\n",
    "    \n",
    "    # save model\n",
    "    if Val_Losses[-1] < best_val_loss:\n",
    "        best_val_loss = Val_Losses[-1]\n",
    "        print('updated best val loss:', best_val_loss)\n",
    "        print('Save model weights to...', output_dir)\n",
    "        torch.save(model.state_dict(), \n",
    "                   os.path.join(output_dir, 'vanilla_rnn-best.pth'.format(epoch + 1, itr + 1)))\n",
    "\n",
    "print('It took: {} s'.format(time.time() - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2.4 Evaluation [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluation code\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "smoother = SmoothingFunction()\n",
    "\n",
    "def caption_generator(model, images, vocab, img_ids, captions, mode='Deterministic', temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate captions.\n",
    "    :param mode:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sample_idxs = model.sample_generate(images, mode=mode,\n",
    "                                        temperature=temperature).data.cpu().numpy()  # [N, max_length]\n",
    "    for i, sentence in enumerate(sample_idxs):  # every sentence in this batch\n",
    "        sentence_caption = ''\n",
    "        for word_idx in sentence:\n",
    "            word = vocab.idx2word[word_idx]\n",
    "            if word != '<start>' and word != '<end>':\n",
    "                if word == '.':\n",
    "                    sentence_caption += '.'\n",
    "                else:\n",
    "                    sentence_caption += word + ' '\n",
    "            if word == '<end>':\n",
    "                break\n",
    "        captions.append({'caption': sentence_caption})\n",
    "        # captions.append(sentence_caption)\n",
    "\n",
    "    return captions\n",
    "\n",
    "def run_test(model, data_loader, vocab, mode='Deterministic', temperature=1.0):\n",
    "    \"\"\"\n",
    "    Run your model on the test set.\n",
    "    Inputs:\n",
    "    :param model: the model you use\n",
    "    :param data_loader: the data_loader\n",
    "    :param mode: use 'deterministic' or 'stochastic'\n",
    "    Outputs:\n",
    "    :param predictions\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for itr, (images, captions, lengths) in enumerate(tqdm(data_loader)):\n",
    "        images = Variable(images).to(device)\n",
    "        captions = Variable(captions).to(device)\n",
    "        outputs = model(images, captions, lengths)\n",
    "        \n",
    "        img_ids = list(range(itr * data_loader.batch_size, (itr + 1) * data_loader.batch_size))\n",
    "        predictions = caption_generator(model, images, vocab, img_ids, \n",
    "                                        predictions, mode=mode, temperature=temperature)\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "def evaluation(model, vocab, data_path=path_to_homework + '/flickr30k_images/', mode='Deterministic', temperature=1.0,\n",
    "               split='test'):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of your model on the test set using BLEU scores.\n",
    "    Inputs:\n",
    "    :param model: the model you use\n",
    "    :param weight_path: the directory to the weights of your model\n",
    "    :param vocab: vocabulary\n",
    "    :param data_path: the directory to the dataset\n",
    "    :param mode: use 'deterministic' or 'stochastic'\n",
    "    Outputs:\n",
    "    :param predictions\n",
    "    \"\"\"\n",
    "    # data loader\n",
    "    test_data_loader = get_loader(root=path_to_homework + '/flickr30k_images/', split=split, vocab=vocab, \n",
    "                                  transform=transform, batch_size=8, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # run your model on the test set\n",
    "    print('Run on the test set...')\n",
    "    preds = run_test(model, test_data_loader, vocab, mode, temperature)\n",
    "    \n",
    "    # load the groundtruth\n",
    "    gt = test_data_loader.dataset.annos\n",
    "    \n",
    "    # evaluate the performance using BLEU score\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    score3 = 0\n",
    "    score4 = 0\n",
    "    \n",
    "    print('Computing BLEU')\n",
    "    for itr in tqdm(range(len(gt))):\n",
    "        candidate = preds[itr]['caption']\n",
    "        reference = [sent['raw'] for sent in gt[itr]['sentences']]\n",
    "        score1 += sentence_bleu(reference, candidate, weights=(1, 0, 0, 0), smoothing_function=smoother.method1)\n",
    "        score2 += sentence_bleu(reference, candidate, weights=(0, 1, 0, 0), smoothing_function=smoother.method1)\n",
    "        score3 += sentence_bleu(reference, candidate, weights=(0, 0, 1, 0), smoothing_function=smoother.method1)\n",
    "        score4 += sentence_bleu(reference, candidate, weights=(0, 0, 0, 1), smoothing_function=smoother.method1)\n",
    "    \n",
    "    bleu1 = 100 * score1/len(gt)\n",
    "    bleu2 = 100 * score2/len(gt)\n",
    "    bleu3 = 100 * score3/len(gt)\n",
    "    bleu4 = 100 * score4/len(gt)\n",
    "    \n",
    "    return bleu1, bleu2, bleu3, bleu4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test your outputs in the **Deterministic** way by using BLEU scores. You should at achieve a BLEU 4 of 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate your model using BLEU score. Use Deterministic mode.\n",
    "\n",
    "## Image transformation\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        #     transforms.RandomCrop(224, pad_if_needed=True),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                             std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "## Evaluate your model using BLEU score. Use Deterministic mode\n",
    "model = Vanilla_rnn(vocab_size=len(vocab), emb_dim=emb_dim, hidden_dim=hidden_dim, \n",
    "                   num_layers=1, dropout=dropout).to(device)  # build a model\n",
    "model.load_state_dict(torch.load(path_to_homework + '/checkpoints/rnn/vanilla_rnn-best.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "bleu1, bleu2, bleu3, bleu4 = evaluation(model, vocab, mode='Deterministic')\n",
    "print(\"BLEU 1:{}, BLEU 2:{}, BLEU 3:{}, BLEU 4:{}\".format(bleu1, bleu2, bleu3, bleu4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try different temperatures (e.g. 0.1, 0.2, 0.5, 1.0, 1.5, 2, etc.) during the generation. Report BLEU scores for at least 3 different temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use at least 3 different temperatures to generate captions on the test set. Report the BLEU scores.\n",
    "# Your code here\n",
    "\n",
    "# End of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 Variations [55 pts]\n",
    "## Section 3.1 LSTM [35 pts]\n",
    "## Section 3.1.1 Decoder: LSTM [5 pts]\n",
    "This time, replace the RNN module with an LSTM module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers=1, dropout=0):\n",
    "        \"\"\"\n",
    "        Use LSTM as decoder for captions.\n",
    "        :param emb_dim: Embedding dimensions.\n",
    "        :param hidden_dim: Hidden states dimensions.\n",
    "        :param num_layers: Number of LSTM layers.\n",
    "        :param vocab_size: The size of Vocabulary.\n",
    "        :param dropout: dropout probability\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        #############Your code############\n",
    "        # you need to implement a LSTM for the decoder. Take a look at the official documentation.\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM\n",
    "        \n",
    "        # one-hot encoding + linear layer\n",
    "        \n",
    "        # LSTM network\n",
    "        \n",
    "        # output layer\n",
    "\n",
    "    def forward(self, encode_features, captions, lengths):\n",
    "        \"\"\"\n",
    "        Feed forward to generate captions.\n",
    "        :param encode_features: output of encoder, size [N, emb_dim]\n",
    "        :param captions: captions, size [N, max(lengths)]\n",
    "        :param lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "        \"\"\"\n",
    "        #############Your Code###################\n",
    "        # compute the embedding using one-hot technique and linear function\n",
    "        \n",
    "        # concatenate the encoded features from encoder and embeddings\n",
    "                \n",
    "        # feed into RNN\n",
    "        \n",
    "        # output layer\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers=1, dropout=0):\n",
    "        \"\"\"\n",
    "        Encoder-decoder vanilla RNN.\n",
    "        :param vocab_size: the size of Vocabulary.\n",
    "        :param emb_dim: the dimensions of word embedding.\n",
    "        :param hidden_dim: the dimensions of hidden units.\n",
    "        :param num_layers: the number of RNN layers.\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.max_length = self.decoder.max_length\n",
    "        #########Your Code################\n",
    "        # Encoder: ResNet-50\n",
    "\n",
    "        # Decoder: LSTM\n",
    "\n",
    "    def forward(self, x, captions, lengths):\n",
    "        \"\"\"\n",
    "        Feed forward.\n",
    "        :param x: Images, [N, 3, H, W]\n",
    "        :param captions: encoded captions, [N, max(lengths)]\n",
    "        :param lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "        :return: output logits, usually followed by a softmax layer.\n",
    "        \"\"\"\n",
    "        ##########Your code###################\n",
    "        # forward passing\n",
    "\n",
    "        return x\n",
    "\n",
    "    def sample_generate(self, x, states=None, mode='Deterministic', temperature=5.0):\n",
    "        \"\"\"\n",
    "        Generate samples during the evaluation.\n",
    "        \n",
    "        :param x: input image\n",
    "        :param states: rnn states\n",
    "        :param mode: which mode we use.  \n",
    "         - 'Deterministic': Take the maximum output at each step.\n",
    "         - 'Stochastic': Sample from the probability distribution from the output layer.\n",
    "        :param temperature: will be used in the stochastic mode\n",
    "        :return: sample_idxs. Word indices. We can use vocab to recover the sentence.\n",
    "        \"\"\"\n",
    "        sample_idxs = []\n",
    "        #################Your Code##################\n",
    "        # compute the encoded features\n",
    "        \n",
    "        # decide which mode we use\n",
    "        if mode == 'Deterministic':\n",
    "            \n",
    "        elif mode == 'Stochastic':\n",
    "            \n",
    "            \n",
    "        return sample_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1.2 Training [10 pts]\n",
    "Use the same set of hyper-parameters (hidden units, optimizer, learning rate etc.) for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some hyperparameters, you can change them\n",
    "## training parameters\n",
    "batch_size = 256\n",
    "lr = 1e-2\n",
    "num_epochs = 50\n",
    "weight_decay = 0.0\n",
    "log_step = 50\n",
    "\n",
    "## network architecture\n",
    "emb_dim = 1024\n",
    "hidden_dim = 256\n",
    "num_layers = 1 # number of RNN layers\n",
    "dropout = 0.0\n",
    "\n",
    "## image transformation\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        #     transforms.RandomCrop(224, pad_if_needed=True),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                             std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "## Output directory\n",
    "output_dir = path_to_homework + '/checkpoints/lstm/'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training code here\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_data_loader = get_loader(root=path_to_homework + '/flickr30k_images/', split='train', vocab=vocab,\n",
    "                               transform=transform, batch_size=batch_size, shuffle=True, num_workers=12)\n",
    "val_data_loader = get_loader(root=path_to_homework + '/flickr30k_images/', split='val', vocab=vocab,\n",
    "                             transform=transform, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "model = LSTM(vocab_size=len(vocab), emb_dim=emb_dim, hidden_dim=hidden_dim, \n",
    "                   num_layers=1, dropout=dropout).to(device)  # build a model\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # CE loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)  # optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "                                      step_size=5,\n",
    "                                      gamma=0.5)  # decay LR by a factor of 0.5 every 10 epochs. You can change this\n",
    "\n",
    "# logs\n",
    "Train_Losses = []  # record average training loss each epoch\n",
    "Val_Losses = []   # record average validation loss each epoch\n",
    "total_step = len(train_data_loader)  # number of iterations each epoch\n",
    "best_val_loss = np.inf\n",
    "\n",
    "# start training\n",
    "print('Start training...')\n",
    "import time\n",
    "tic = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    print('Switch to training...')\n",
    "    model.train()\n",
    "    Train_loss_iter = []  # record the the training loss each iteration\n",
    "    for itr, (images, captions, lengths) in enumerate(train_data_loader):\n",
    "        ########Your Code###########\n",
    "        # train your model\n",
    "        \n",
    "        # record the training loss\n",
    "        \n",
    "        \n",
    "        # print log info\n",
    "        if itr % log_step == 0:\n",
    "            # print current loss and perplexity\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                      .format(epoch, num_epochs, itr, total_step, loss.item(), np.exp(loss.item())))\n",
    "    scheduler.step()\n",
    "    Train_Losses.append(np.mean(Train_loss_iter))\n",
    "    np.save(os.path.join(output_dir, 'TrainingLoss_lstm.npy'), Train_Losses)  # save the training loss\n",
    "    \n",
    "    model.eval()\n",
    "    # (optional) generate a sample during the training, you can use deterministic mode\n",
    "    # Your code\n",
    "    \n",
    "    \n",
    "    # validation\n",
    "    Val_Losses.append(val(model, val_data_loader, vocab))\n",
    "    np.save(os.path.join(output_dir, 'ValLoss_lstm.npy'), Val_Losses) # save the val loss\n",
    "    \n",
    "    # save model\n",
    "    if Val_Losses[-1] < best_val_loss:\n",
    "        best_val_loss = Val_Losses[-1]\n",
    "        print('updated best val loss:', best_val_loss)\n",
    "        print('Save model weights to...', output_dir)\n",
    "        torch.save(model.state_dict(), \n",
    "                   os.path.join(output_dir, 'lstm-best.pth'.format(epoch + 1, itr + 1)))\n",
    "\n",
    "print('It took: {} s'.format(time.time() - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1.3 Evalution [10 pts]\n",
    "Evaluate your model on the test set by perplexity score or BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate your model using BLEU score. Use Deterministic mode.\n",
    "# Your code here\n",
    "\n",
    "# End of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use at least 3 different temperatures to generate captions on the test set. Report the BLEU scores.\n",
    "# Your code here\n",
    "\n",
    "# End of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1.4 Discussion [5 pts]\n",
    "What's the difference between Vanilla RNN and LSTM (training loss, evaluation results, etc)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comments**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2 Using pre-trained word embeddings [20 pts]\n",
    "For now, the decoder uses a word as input by converting it into a fixed size embedding, and our networks learn these word embeddings by training. In this experiment, you will use pre-trained word embeddings like Word2Vec or GloVe in LSTM. If you use Pytorch’s nn.Embedding layer, you can initialize its weights with a matrix containing pre-trained word embeddings for all words in your vocabulary, and freeze the weights (i.e. don’t train this layer). You can find these embeddings online.\n",
    "\n",
    "Some resources:\n",
    "- GloVe: https://nlp.stanford.edu/projects/glove/\n",
    "- Word2Vec: http://jalammar.github.io/illustrated-word2vec/\n",
    "\n",
    "In case you don't know how to get one, we've already provided a light GloVe embedding: wm_06.npy, which can produce 300-d word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2.1 Encoder-decoder [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, pretrained_emb, num_layers=1, dropout=0):\n",
    "        \"\"\"\n",
    "        Use LSTM as decoder for captions.\n",
    "        :param emb_dim: Embedding dimensions.\n",
    "        :param hidden_dim: Hidden states dimensions.\n",
    "        :param pretrained_emb: the path to the pretrained embedding\n",
    "        :param num_layers: Number of LSTM layers.\n",
    "        :param vocab_size: The size of Vocabulary.\n",
    "        :param dropout: dropout probability\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.max_length = 30  # in case it's trapped\n",
    "        ###### Your Code#########\n",
    "        # load pre-trained embedding weights and freeze this layer\n",
    "        \n",
    "        # lstm network\n",
    "        \n",
    "        # output layer\n",
    "    \n",
    "    def forward(self, encode_features, captions, lengths):\n",
    "        \"\"\"\n",
    "        Feed forward to generate captions.\n",
    "        :param encode_features: output of encoder, size [N, emb_dim]\n",
    "        :param captions: captions, size [N, max(lengths)]\n",
    "        :param lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "        \"\"\"\n",
    "        #############Your Code###################\n",
    "        # compute the embedding using one-hot technique and linear function\n",
    "        \n",
    "        # concatenate the encoded features from encoder and embeddings\n",
    "                \n",
    "        # feed into RNN\n",
    "        \n",
    "        # output layer\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, pretrained_emb, num_layers=1, dropout=0):\n",
    "        \"\"\"\n",
    "        Encoder-decoder baseline.\n",
    "        :param vocab_size: the size of Vocabulary.\n",
    "        :param emb_dim: the dimensions of word embedding.\n",
    "        :param hidden_dim: the dimensions of hidden units.\n",
    "        :param pretrained_emb: the path to the pretrained embedding\n",
    "        :param num_layers: the number of LSTM layers.\n",
    "        :param dropout: dropout probability.\n",
    "        \"\"\"\n",
    "        super(Word_embeddings, self).__init__()\n",
    "        self.max_length = self.decoder.max_length\n",
    "        #########Your Code################\n",
    "        # Encoder: ResNet-50\n",
    "\n",
    "        # Decoder: LSTM\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x, captions, lengths):\n",
    "        \"\"\"\n",
    "        Feed forward.\n",
    "        :param x: Images, [N, 3, H, W]\n",
    "        :param captions: encoded captions, [N, max(lengths)]\n",
    "        :param lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "        :return: output logits, usually followed by a softmax layer.\n",
    "        \"\"\"\n",
    "        ##########Your code###################\n",
    "        # forward passing\n",
    "\n",
    "        return x\n",
    "\n",
    "    def sample_generate(self, x, states=None, mode='Deterministic', temperature=5.0):\n",
    "        \"\"\"\n",
    "        Generate samples.\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sample_idxs = []\n",
    "        #################Your Code##################\n",
    "        # compute the encoded features\n",
    "        \n",
    "        # decide which mode we use\n",
    "        if mode == 'Deterministic':\n",
    "            \n",
    "        elif mode == 'Stochastic':\n",
    "            \n",
    "            \n",
    "        return sample_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2.2 Training [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some hyperparameters, you can change them\n",
    "## training parameters\n",
    "batch_size = 256\n",
    "lr = 1e-2\n",
    "num_epochs = 50\n",
    "weight_decay = 0.0\n",
    "log_step = 50\n",
    "\n",
    "## network architecture\n",
    "emb_dim = 300\n",
    "hidden_dim = 256\n",
    "num_layers = 1 # number of RNN layers\n",
    "dropout = 0.0\n",
    "\n",
    "## image transformation\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        #     transforms.RandomCrop(224, pad_if_needed=True),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                             std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "## Output directory\n",
    "output_dir = path_to_homework + '/checkpoints/pretrained_emb/'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training code here\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_data_loader = get_loader(root=path_to_homework + '/flickr30k_images/', split='train', vocab=vocab,\n",
    "                               transform=transform, batch_size=batch_size, shuffle=True, num_workers=12)\n",
    "val_data_loader = get_loader(root=path_to_homework + '/flickr30k_images/', split='val', vocab=vocab,\n",
    "                             transform=transform, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "# pretrained embedding weights\n",
    "pre_emb_path = '.'  # type the path to the pretrained embedding you find\n",
    "\n",
    "model = Word_embeddings(vocab_size=len(vocab), emb_dim=emb_dim, hidden_dim=hidden_dim, pretrained_emb=pre_emb_path,\n",
    "                   num_layers=1, dropout=dropout).to(device)  # build a model\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # CE loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)  # optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "                                      step_size=5,\n",
    "                                      gamma=0.5)  # decay LR by a factor of 0.5 every 10 epochs. You can change this\n",
    "\n",
    "# logs\n",
    "Train_Losses = []  # record average training loss each epoch\n",
    "Val_Losses = []   # record average validation loss each epoch\n",
    "total_step = len(train_data_loader)  # number of iterations each epoch\n",
    "best_val_loss = np.inf\n",
    "\n",
    "# start training\n",
    "print('Start training...')\n",
    "import time\n",
    "tic = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    print('Switch to training...')\n",
    "    model.train()\n",
    "    Train_loss_iter = []  # record the the training loss each iteration\n",
    "    for itr, (images, captions, lengths) in enumerate(train_data_loader):\n",
    "        ########Your Code###########\n",
    "        # train your model\n",
    "        \n",
    "        # record the training loss\n",
    "        \n",
    "        \n",
    "        # print log info\n",
    "        if itr % log_step == 0:\n",
    "            # print current loss and perplexity\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                      .format(epoch, num_epochs, itr, total_step, loss.item(), np.exp(loss.item())))\n",
    "    scheduler.step()\n",
    "    Train_Losses.append(np.mean(Train_loss_iter))\n",
    "    np.save(os.path.join(output_dir, 'TrainingLoss_lstm.npy'), Train_Losses)  # save the training loss\n",
    "    \n",
    "    model.eval()\n",
    "    # (optional) generate a sample during the training, you can use deterministic mode\n",
    "    # Your code\n",
    "    \n",
    "    \n",
    "    # validation\n",
    "    Val_Losses.append(val(model, val_data_loader, vocab))\n",
    "    np.save(os.path.join(output_dir, 'ValLoss_lstm.npy'), Val_Losses) # save the val loss\n",
    "    \n",
    "    # save model\n",
    "    if Val_Losses[-1] < best_val_loss:\n",
    "        best_val_loss = Val_Losses[-1]\n",
    "        print('updated best val loss:', best_val_loss)\n",
    "        print('Save model weights to...', output_dir)\n",
    "        torch.save(model.state_dict(), \n",
    "                   os.path.join(output_dir, 'pretrain-best.pth'.format(epoch + 1, itr + 1)))\n",
    "\n",
    "print('It took: {} s'.format(time.time() - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2.3 Evaluation [3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate your model using BLEU score. Use Deterministic mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2.4 Discussion [2 pts]\n",
    "Compared to index embeddings, do pretrained embeddings improve the performance? Try to explain it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Comments**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines for Downloading PDF in Google Colab\n",
    "- Run below cells only in Google Colab, Comment out in case of Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run below two lines (in google colab), installation steps to get .pdf of the notebook\n",
    "\n",
    "!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n",
    "!pip install pypandoc\n",
    "\n",
    "# After installation, comment above two lines and run again to remove installation comments from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find path to your notebook file in drive and enter in below line\n",
    "\n",
    "!jupyter nbconvert --to PDF \"your_notebook_path_here/DL_Assignment_4.ipynb\"\n",
    "\n",
    "#Example: \"/content/drive/My Drive/DL_Fall_2020/Assignment_4/DL_Assignment_4.ipynb\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
