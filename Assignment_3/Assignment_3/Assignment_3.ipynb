{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE-6524 / CS-6524 Deep Learning\n",
    "# Assignment 3 [100 pts]\n",
    "\n",
    "In this assignment, **you need to complete the Yolo loss function, and train an object detector. Yay!**\n",
    "\n",
    "## Submission guideline for the coding part (Jupyter Notebook)\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook\n",
    "2. Please make sure to have entered your Virginia Tech PID below\n",
    "3. Once you've completed everything (make sure output for all cells are visible), select File -> Download as -> PDF via LaTeX\n",
    "4. Look at the PDF file and make sure all your solutions are displayed correctly there \n",
    "7. Zip all the files along with this notebook (Please don't include the data). Name it as Assignment_3_Code_[YOUR PID NUMBER].zip\n",
    "8. Name your PDF file as Assignment_2_NB_[YOUR PID NUMBER].pdf\n",
    "9. **<span style=\"color:blue\"> Submit your zipped file and the PDF SEPARATELY**</span>\n",
    "\n",
    "Note: if facing issues with step 3 refer: https://pypi.org/project/notebook-as-pdf/\n",
    "\n",
    "## Submission guideline for the coding part (Google Colab)\n",
    "\n",
    "1. Click the Save button at the top of the Notebook\n",
    "2. Please make sure to have entered your Virginia Tech PID below\n",
    "3. Follow last two cells in this notebook for guidelines to download pdf file of this notebook\n",
    "4. Look at the PDF file and make sure all your solutions are displayed correctly there \n",
    "5. Zip all the files along with this notebook (Please don't include the data). Name it as Assignment_2_Code_[YOUR PID NUMBER].zip\n",
    "6. Name your PDF file as Assignment_2_NB_[YOUR PID NUMBER].pdf\n",
    "7. **<span style=\"color:blue\"> Submit your zipped file and the PDF SEPARATELY**</span>\n",
    "\n",
    "**While you are encouraged to discuss with your peers, <span style=\"color:blue\">all work submitted is expected to be your own.</span> <span style=\"color:red\">If you use any information from other resources (e.g. online materials), you are required to cite it below you VT PID. Any violation will result in a 0 mark for the assignment.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please Write Your VT PID Here: \n",
    "### Reference (if any):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you would need to use **Python 3.6+** along with the following packages:\n",
    "```\n",
    "1. pytorch 1.2\n",
    "2. torchvision\n",
    "3. numpy\n",
    "4. matplotlib\n",
    "5. tqdm (for better, cuter progress bar. Yay!)\n",
    "```\n",
    "To install pytorch, please follow the instructions on the [Official website](https://pytorch.org/). In addition, the [official document](https://pytorch.org/docs/stable/) could be very helpful when you want to find certain functionalities. \n",
    "\n",
    "\n",
    "<span style=\"color:red\">Note that, on a high-end GPU, it sill takes 3-4 hours to train. **SO START EARLY. IT'S IMPOSSIBLE TO FINISH IT AT THE LAST MINUTE!**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To select GPU in Google Colab:\n",
    "- go to **Edit -> Notebook settings -> Hardware accelerator -> GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# modify \"customized_path_to_homework\", path of folder in drive, where you uploaded your homework\n",
    "customized_path_to_homework = \"/content/drive/My Drive/DL_Fall_2020/Assignment_3\"\n",
    "sys.path.append(customized_path_to_homework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to download dataset, give path to the download.sh file from your drive\n",
    "!sh /content/drive/My\\ Drive/DL_Fall_2020/Assignment_3/download_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy and place downloaded dataset to your drive. To access dataset multiple times, no need to download everytime you open colab.\n",
    "!cp -r /content/VOCdevkit_2007 enter_path_to_your_folder_here (example: /content/drive/My\\ Drive/DL_Fall_2020/Assignment_3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "from resnet_yolo import resnet50\n",
    "from dataset import VocDetectorDataset\n",
    "from eval_voc import evaluate\n",
    "from predict import predict_image\n",
    "from config import VOC_CLASSES, COLORS\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(torch.cuda.get_device_name()) # GPU name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You Only Look Once: Unified, Real-Time Object Detection \n",
    "In this assignment, you need to implement the loss function and train the **YOLO object detector** (specfically, YOLO-v1). Here we provide a list of recommend readings for you:\n",
    "- [YOLO original paper](https://arxiv.org/pdf/1506.02640.pdf) (recommended)\n",
    "- [Object detection methods](http://slazebni.cs.illinois.edu/fall18/lec09_detection.pdf) (Slides)\n",
    "- [Great post about YOLO](https://medium.com/adventures-with-deep-learning/yolo-v1-part-1-cfb47135f81f) on Medium\n",
    "- [Differences between YOLO, YOLOv2 and YOLOv3\n",
    "](https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088)\n",
    "- [Great explanation of the Yolo Loss function](https://stats.stackexchange.com/questions/287486/yolo-loss-function-explanation)\n",
    "\n",
    "We adopt a variant of YOLO, which:\n",
    "1. Use pretrained ResNet50 classifier as detector backbone. The pretrained model is offered in `torchvision.models`.\n",
    "2. Instead of using a $7\\times7$ detection grid, we use $14\\times14$ to get a more finegrained detection.\n",
    "\n",
    "In general, the backbone models are usually pretrained on ImageNet dataset (> 1 million images) with numerous classes. As a result, having these pretrained backbone can greatly shorten the required training time, as well as improve the performance. <span style=\"color:red\">**But still, it takes at least 3-4 hours to train, not to mention that you might need to debug after one training run. So START EARLY, DON'T GO #YOLO!**</span>\n",
    "\n",
    "<img src=\"figure/example.png\" width=\"450\">\n",
    "You are supposed to get a reasonable detector (like the ... above?) after training the model correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO network hyperparameters\n",
    "B = 2  # number of bounding box predictions per cell\n",
    "S = 14  # width/height of network output grid (larger than 7x7 from paper since we use a different network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pretrained ResNet classifier\n",
    "Load the pretrained classifier. By default, it would use the pretrained model provided by `Pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_network_path = None\n",
    "pretrained = True\n",
    "\n",
    "# use to load a previously trained network\n",
    "if load_network_path is not None:\n",
    "    print('Loading saved network from {}'.format(load_network_path))\n",
    "    net = resnet50().to(device)\n",
    "    net.load_state_dict(torch.load(load_network_path))\n",
    "else:\n",
    "    print('Load pre-trained model')\n",
    "    net = resnet50(pretrained=pretrained).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic hyperparameter settings that you probably don't have to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "batch_size = 24\n",
    "\n",
    "# Yolo loss component coefficients (as given in Yolo v1 paper)\n",
    "lambda_coord = 5\n",
    "lambda_noobj = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the YOLO-v1 loss [80 pts]\n",
    "Now, you have to implement the `YoloLoss` for training your object detector. Please read closely to the [YOLO original paper](https://arxiv.org/pdf/1506.02640.pdf) so that you can implement it.\n",
    "\n",
    "In general, there are 4 components in the YOLO loss. Consider that we have our prediction grid of size$(N, S, S, 5B+c)$ ( (x, y, w, h, C) for each bounding box, and c is the number of classes), where $N$ is the batch size, $S$ is the grid size, $B$ is the number of bounding boxes. We have :\n",
    "1. Bounding box regression loss on the bounding box$(x, y, w, h)$\n",
    "    - $l_{coord}=\\sum_{i=0}^{S^2}\\sum_{j=0}^B\\mathbb{1}^{obj}_{ij}\\left[(x_i-\\hat{x}_i)^2+(y_i-\\hat{y}_i)^2\\right]$ + $\\sum_{i=0}^{S^2}\\sum_{j=0}^B\\mathbb{1}^{obj}_{ij}\\left[(\\sqrt{w_i}-\\sqrt{\\hat{w}_i})^2+(\\sqrt{h_i}-\\sqrt{\\hat{h}_i})^2\\right]$\n",
    "    - $\\mathbb{1}^{obj}_{ij}$: equals to 1 when object appears in cell $i$, and the bounding box $j$ is responsible for the prediction. 0 otherwise.\n",
    "2. Contain object loss on the confidence prediction $c$ (only calculate for those boxes that actually have objects)\n",
    "    - $l_{contain}=\\sum_{i=0}^{S^2}\\sum_{j=0}^B\\mathbb{1}^{obj}_{ij}(C_i-\\hat{C}_i)^2$\n",
    "    - $C_i$ the predicted confidence score for cell $i$ from predicted box $j$\n",
    "    - For each grid cell, you only calculate the contain object loss for the predicted bounding box that has maximum overlap (iou) with the gruond truth box.\n",
    "    - We say that this predicted box with maximum iou is **responsible** for the prediction.\n",
    "3. No object loss on the confidence prediction $c$ (only calculate for those boxes that don't have objects)\n",
    "    - $l_{noobj}=\\sum_{i=0}^{S^2}\\sum_{j=0}^B\\mathbb{1}^{noobj}_{ij}(C_i-\\hat{C}_i)^2$\n",
    "    - $\\mathbb{1}^{obj}_{ij}$: equals to 1 when **no object appears** in cell $i$.\n",
    "4. Classification error loss.\n",
    "    - $l_{class}=\\sum_{i=0}^{S^2}\\mathbb{1}_i^{obj}\\sum_{c\\in classes}\\left(p_i(c)-\\hat{p_i}(c)\\right)^2$\n",
    "    - $p_i(c)$ is the predicted score for class $c$\n",
    "    \n",
    "Putting them together, we get the yolo loss:\n",
    "\\begin{equation}\n",
    "yolo=\\lambda_{coord}l_{coord}+l_{contain}+\\lambda_{noobj}l_{noobj}+l_{class}\n",
    "\\end{equation}\n",
    "where $\\lambda$ are hyperparameters. We have provided detailed comments to guide you through implementing the loss. So now, please complete the YoloLoss in the code block below. **If you have any problem with regard to implementation, post and discuss it on Piazza.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    " \n",
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self,S,B,l_coord,l_noobj):\n",
    "        super(YoloLoss,self).__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.l_coord = l_coord\n",
    "        self.l_noobj = l_noobj\n",
    " \n",
    "    def compute_iou(self, box1, box2):                                                                                                                                                             \n",
    "        \"\"\"Compute the intersection over union of two set of boxes, each box is [x1,y1,x2,y2].\n",
    "        Args:\n",
    "          box1: (tensor) bounding boxes, sized [N,4].\n",
    "          box2: (tensor) bounding boxes, sized [M,4].\n",
    "        Return:\n",
    "          (tensor) iou, sized [N,M].\n",
    "        \"\"\"\n",
    "        N = box1.size(0)\n",
    "        M = box2.size(0)\n",
    " \n",
    "        lt = torch.max(\n",
    "            box1[:,:2].unsqueeze(1).expand(N,M,2),  # [N,2] -> [N,1,2] -> [N,M,2]\n",
    "            box2[:,:2].unsqueeze(0).expand(N,M,2),  # [M,2] -> [1,M,2] -> [N,M,2]\n",
    "        )   \n",
    " \n",
    "        rb = torch.min(\n",
    "            box1[:,2:].unsqueeze(1).expand(N,M,2),  # [N,2] -> [N,1,2] -> [N,M,2]\n",
    "            box2[:,2:].unsqueeze(0).expand(N,M,2),  # [M,2] -> [1,M,2] -> [N,M,2]\n",
    "        )   \n",
    " \n",
    "        wh = rb - lt  # [N,M,2]\n",
    "        wh[wh<0] = 0  # clip at 0\n",
    "        inter = wh[:,:,0] * wh[:,:,1]  # [N,M]\n",
    " \n",
    "        area1 = (box1[:,2]-box1[:,0]) * (box1[:,3]-box1[:,1])  # [N,]\n",
    "        area2 = (box2[:,2]-box2[:,0]) * (box2[:,3]-box2[:,1])  # [M,]\n",
    "        area1 = area1.unsqueeze(1).expand_as(inter)  # [N,] -> [N,1] -> [N,M]\n",
    "        area2 = area2.unsqueeze(0).expand_as(inter)  # [M,] -> [1,M] -> [N,M]\n",
    " \n",
    "        iou = inter / (area1 + area2 - inter)\n",
    "        return iou \n",
    "    \n",
    "    def get_class_prediction_loss(self, classes_pred, classes_target):\n",
    "        \"\"\" \n",
    "        Parameters:\n",
    "        classes_pred : (tensor) size (batch_size, S, S, 20)                                                                                                                                        \n",
    "        classes_target : (tensor) size (batch_size, S, S, 20)\n",
    "         \n",
    "        Returns:\n",
    "        class_loss : scalar\n",
    "        \"\"\"\n",
    "        ##### CODE #####\n",
    "\n",
    "        ##### CODE #####\n",
    "        return class_loss\n",
    "         \n",
    "         \n",
    "    def get_regression_loss(self, box_pred_response, box_target_response):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        box_pred_response : (tensor) size (-1, 5)\n",
    "        box_target_response : (tensor) size (-1, 5)\n",
    "        Note : -1 corresponds to ravels the tensor into the dimension specified \n",
    "        See : https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view_as\n",
    "         \n",
    "        Returns:\n",
    "        reg_loss : scalar\n",
    "        \"\"\"\n",
    "        ##### CODE #####\n",
    "        \n",
    "        ##### CODE #####\n",
    "        return reg_loss\n",
    "         \n",
    "    def get_contain_object_loss(self, box_pred_response, box_target_response_iou):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        box_pred_response : (tensor) size ( -1 , 5)\n",
    "        box_target_response_iou : (tensor) size ( -1 , 5)\n",
    "        Note : -1 corresponds to ravels the tensor into the dimension specified \n",
    "        See : https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view_as\n",
    "         \n",
    "        Returns:\n",
    "        contain_loss : scalar\n",
    "        \"\"\"\n",
    "        ##### CODE #####\n",
    "        \n",
    "        ##### CODE #####\n",
    "        return contain_loss\n",
    "         \n",
    "    def get_no_object_loss(self, target_tensor, pred_tensor, no_object_mask):\n",
    "        \"\"\"                                                                                                                                                                                        \n",
    "        Parameters:\n",
    "        target_tensor : (tensor) size (batch_size, S , S, 30)\n",
    "        pred_tensor : (tensor) size (batch_size, S , S, 30)\n",
    "        no_object_mask : (tensor) size (batch_size, S , S)\n",
    "         \n",
    "        Returns:\n",
    "        no_object_loss : scalar\n",
    "         \n",
    "        Hints:\n",
    "        1) Create 2 tensors no_object_prediction and no_object_target which only have the \n",
    "        values which have no object. \n",
    "        2) Have another tensor no_object_prediction_mask of the same size such that \n",
    "        mask with respect to both confidences of bounding boxes set to 1. \n",
    "        3) Create 2 tensors which are extracted from no_object_prediction and no_object_target using\n",
    "        the mask created above to find the loss. \n",
    "        \"\"\"\n",
    "        ##### CODE #####\n",
    "\n",
    "        ##### CODE #####\n",
    "        return no_object_loss\n",
    "          \n",
    "    def find_best_iou_boxes(self, box_target, box_pred):\n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "        box_target : (tensor)  size (-1, 5)\n",
    "        box_pred : (tensor) size (-1, 5)\n",
    "        Note : -1 corresponds to ravels the tensor into the dimension specified \n",
    "        See : https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view_as\n",
    "         \n",
    "        Returns: \n",
    "        box_target_iou: (tensor)\n",
    "        contains_object_response_mask : (tensor)\n",
    "         \n",
    "        Hints:\n",
    "        1) Find the iou's of each of the 2 bounding boxes of each grid cell of each image.\n",
    "        2) Set the corresponding contains_object_response_mask of the bounding box with the max iou\n",
    "        of the 2 bounding boxes of each grid cell to 1.\n",
    "        3) For finding iou's use the compute_iou function\n",
    "        4) Before using compute preprocess the bounding box coordinates in such a way that \n",
    "        if for a Box b the coordinates are represented by [x, y, w, h] then \n",
    "        x, y = x/S - 0.5*w, y/S - 0.5*h ; w, h = x/S + 0.5*w, y/S + 0.5*h\n",
    "        Note: Over here initially x, y are the center of the box and w,h are width and height. \n",
    "        We perform this transformation to convert the correct coordinates into bounding box coordinates.\n",
    "        5) Set the confidence of the box_target_iou of the bounding box to the maximum iou\n",
    "        \"\"\"\n",
    "        ##### CODE #####\n",
    "\n",
    "        ##### CODE #####\n",
    "        return box_target_iou, coo_response_mask\n",
    "         \n",
    "    def forward(self, pred_tensor,target_tensor):\n",
    "        '''\n",
    "        pred_tensor: (tensor) size(batchsize,S,S,Bx5+20=30)\n",
    "                      where B - number of bounding boxes this grid cell is a part of = 2\n",
    "                            5 - number of bounding box values corresponding to [x, y, w, h, c]\n",
    "                                where x - x_coord, y - y_coord, w - width, h - height, c - confidence of having an object\n",
    "                            20 - number of classes\n",
    "         \n",
    "        target_tensor: (tensor) size(batchsize,S,S,30)\n",
    "         \n",
    "        Returns:\n",
    "        Total Loss\n",
    "        '''\n",
    "        N = pred_tensor.size(0)\n",
    "         \n",
    "        total_loss = None\n",
    "        # Create 2 tensors contains_object_mask and no_object_mask \n",
    "        # of size (Batch_size, S, S) such that each value corresponds to if the confidence of having \n",
    "        # an object > 0 in the target tensor.\n",
    "\n",
    "        ##### CODE #####\n",
    "        \n",
    "        ##### CODE #####\n",
    "        \"\"\"\n",
    "        Create a tensor contains_object_pred that corresponds to \n",
    "        to all the predictions which seem to confidence > 0 for having an object\n",
    "        Then, split this tensor into 2 tensors :                                                                                                                                                       \n",
    "        1) bounding_box_pred : Contains all the Bounding box predictions (x, y, w, h, c) of all grid \n",
    "                                cells of all images\n",
    "        2) classes_pred : Contains all the class predictions for each grid cell of each image\n",
    "        Hint : Use contains_object_mask\n",
    "        \"\"\" \n",
    "        ##### CODE #####\n",
    "\n",
    "        ##### CODE #####                   \n",
    "        \"\"\"\n",
    "        # Similarly, create 2 tensors bounding_box_target and classes_target\n",
    "        # using the contains_object_mask.\n",
    "        \"\"\"\n",
    "        ##### CODE #####\n",
    "        \n",
    "        ##### CODE #####\n",
    "        \n",
    "        #Compute the No object loss here\n",
    "        # Instruction: finish your get_no_object_loss\n",
    "        ##### CODE #####\n",
    "        \n",
    "        ##### CODE #####\n",
    "        \"\"\"\n",
    "        # Compute the iou's of all bounding boxes and the mask for which bounding box \n",
    "        # of 2 has the maximum iou the bounding boxes for each grid cell of each image.\n",
    "        # Instruction: finish your find_best_iou_boxes and use it.\n",
    "        \"\"\"\n",
    "        ##### CODE #####\n",
    "\n",
    "        ##### CODE #####\n",
    "        \"\"\"        \n",
    "        # Create 3 tensors :\n",
    "        # 1) box_prediction_response - bounding box predictions for each grid cell which has the maximum iou\n",
    "        # 2) box_target_response_iou - bounding box target ious for each grid cell which has the maximum iou\n",
    "        # 3) box_target_response -  bounding box targets for each grid cell which has the maximum iou\n",
    "        # Hint : Use coo_response_mask\n",
    "        \"\"\"\n",
    "        ##### CODE #####\n",
    "        \n",
    "        ##### CODE #####\n",
    "        \"\"\"\n",
    "        # Find the class_loss, containing object loss and regression loss\n",
    "        \"\"\"\n",
    "        ##### CODE #####\n",
    "\n",
    "        ##### CODE #####\n",
    "        return total_loss / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = YoloLoss(S, B, lambda_coord, lambda_noobj)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Pascal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Pascal is a small dataset (5000 in train+val) we have combined the train and val splits to train our detector. This is not typically a good practice, but we will make an exception in this case to be able to get reasonable detection results with a comparatively small object detection dataset. Use `download_data.sh` to download the dataset.\n",
    "\n",
    "The train dataset loader also using a variety of data augmentation techniques including random shift, scaling, crop, and flips. Data augmentation is slightly more complicated for detection dataset since the bounding box annotations must be kept consistent through the transformations.\n",
    "\n",
    "Since the output of the dector network we train is a $(S, S, 5B+c)$ tensor, we use an encoder to convert the original bounding box coordinates into relative grid bounding box coordinates corresponding to the the expected output. We also use a decoder which allows us to convert the opposite direction into image coordinate bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_root_train = customized_path_to_homework + '/VOCdevkit_2007/VOC2007/JPEGImages/'\n",
    "annotation_file_train = customized_path_to_homework + '/voc2007.txt'\n",
    "\n",
    "train_dataset = VocDetectorDataset(root_img_dir=file_root_train,dataset_file=annotation_file_train,train=True, S=S)\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "print('Loaded %d train images' % len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_root_test = customized_path_to_homework + '/VOCdevkit_2007/VOC2007test/JPEGImages/'\n",
    "annotation_file_test = customized_path_to_homework + '/voc2007test.txt'\n",
    "\n",
    "test_dataset = VocDetectorDataset(root_img_dir=file_root_test,dataset_file=annotation_file_test,train=False, S=S)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False,num_workers=4)\n",
    "print('Loaded %d test images' % len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train detector\n",
    "Now, train your detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_loss = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    \n",
    "    # Update learning rate late in training\n",
    "    if epoch == 30 or epoch == 40:\n",
    "        learning_rate /= 10.0\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = learning_rate\n",
    "    \n",
    "    print('\\n\\nStarting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "    print('Learning Rate for this epoch: {}'.format(learning_rate))\n",
    "    \n",
    "    total_loss = 0.\n",
    "    \n",
    "    for i, (images, target) in enumerate(tqdm(train_loader, total=len(train_loader))):\n",
    "        images, target = images.to(device), target.to(device)\n",
    "\n",
    "        pred = net(images)\n",
    "        loss = criterion(pred,target)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch [%d/%d], average_loss: %.4f'\n",
    "            % (epoch+1, num_epochs, total_loss / (i+1)))\n",
    "    \n",
    "    # evaluate the network on the test data\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        net.eval()\n",
    "        for i, (images, target) in enumerate(tqdm(test_loader, total=len(test_loader))):\n",
    "            images, target = images.to(device), target.to(device)\n",
    "\n",
    "            pred = net(images)\n",
    "            loss = criterion(pred,target)\n",
    "            test_loss += loss.item()\n",
    "        test_loss /= len(test_loader)\n",
    "    \n",
    "    if best_test_loss > test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        print('Updating best test loss: %.5f' % best_test_loss)\n",
    "        torch.save(net.state_dict(),'best_detector.pth')\n",
    "\n",
    "    torch.save(net.state_dict(),'detector.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View example predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take a glance at how your detector works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "net.load_state_dict(torch.load('best_detector.pth'))\n",
    "# select random image from train set\n",
    "image_name = random.choice(train_dataset.fnames)\n",
    "image = cv2.imread(os.path.join(file_root_train, image_name))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "threshold = 0.1\n",
    "print('predicting...')\n",
    "print(image.shape)\n",
    "result = predict_image(net, image_name, root_img_directory=file_root_train, threshold=threshold)\n",
    "for left_up, right_bottom, class_name, _, prob in result:\n",
    "    color = COLORS[VOC_CLASSES.index(class_name)]\n",
    "    cv2.rectangle(image, left_up, right_bottom, color, 2)\n",
    "    label = class_name + str(round(prob, 2))\n",
    "    text_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n",
    "    p1 = (left_up[0], left_up[1] - text_size[1])\n",
    "    cv2.rectangle(image, (p1[0] - 2 // 2, p1[1] - 2 - baseline), (p1[0] + text_size[0], p1[1] + text_size[1]),\n",
    "                  color, -1)\n",
    "    cv2.putText(image, label, (p1[0], p1[1] + baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, 8)\n",
    "\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluate on Test [20 pts]\n",
    "\n",
    "To evaluate detection results we use mAP (mean of average precision over each class), You are expected to get an map of at least 49."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_aps = evaluate(net, test_dataset_file=annotation_file_test, threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines for Downloading PDF in Google Colab\n",
    "- Run below cells only in Google Colab, Comment out in case of Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run below two lines (in google colab), installation steps to get .pdf of the notebook\n",
    "\n",
    "!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n",
    "!pip install pypandoc\n",
    "\n",
    "# After installation, comment above two lines and run again to remove installation comments from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find path to your notebook file in drive and enter in below line\n",
    "\n",
    "!jupyter nbconvert --to PDF \"your_notebook_path_here/DL_Assignment_2.ipynb\"\n",
    "\n",
    "#Example: \"/content/drive/My Drive/DL_Fall_2020/Assignment_2/DL_Assignment_2.ipynb\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
