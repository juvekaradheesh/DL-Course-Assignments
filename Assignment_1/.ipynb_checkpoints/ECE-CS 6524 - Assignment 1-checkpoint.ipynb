{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE-6524 / CS-6524 Deep Learning\n",
    "# Assignment 1 [90 pts]\n",
    "\n",
    "In this assignment, **you need to complete the following three sectoins**:\n",
    "1. SVM Classifier\n",
    "    - Calculating loss and gradient\n",
    "2. Linear Classfier with Softmax \n",
    "    - Calculating loss and gradient\n",
    "3. Simple Neural Network with Multilayer Perceptrons\n",
    "    - Read the forward and backward propogation\n",
    "    - Experiment with simple NN\n",
    "\n",
    "This assignment is inspired and adopted from Stanford CS231n, UIUC Deep Learning course and Deep Learning Lab from Professor Vicente Ordonez at the University of Virginia.\n",
    "## Submission guideline\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Please make sure to have entered your Virginia Tech PID below.\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of ll cells).\n",
    "4. Select Cell -> Run All. This will run all the cells in order.\n",
    "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX\n",
    "6. Look at the PDF file and make sure all your solutions are there, displayed correctly. \n",
    "7. Zip BOTH the PDF file and this notebook. Rem\n",
    "8. Submit your zipped file.\n",
    "\n",
    "**While you are encouraged to discuss with your peers, <span style=\"color:blue\">all work submitted is expected to be your own.</span> <span style=\"color:red\">If you use any information from other resources (e.g. online materials), you are required to cite it below you VT PID. Any violation will result in a 0 mark for the assignment.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please Write Your VT PID Here: \n",
    "### Reference (if any):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you would need to use **Python 3.6+** along with the following packages:\n",
    "```\n",
    "1. numupy\n",
    "2. matplotlib\n",
    "3. scipy\n",
    "```\n",
    "All packages above can be installed through `pip install`.\n",
    "\n",
    "In addition, you will need to have [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset in your `cifar10/cifar-10-batches-py` folder. We provide a bash script `cifar10/get_dataset.sh` to help you download and unpack the dataset.\n",
    "\n",
    "Once you are done with the things above, you can proceed to execute the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from data_process import get_CIFAR10_data\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells, we would first load the CIFAR10 dataset, and prepare them for our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change these numbers for experimentation\n",
    "# For submission we will use the default values \n",
    "TRAIN_IMAGES = 49000\n",
    "VAL_IMAGES = 1000\n",
    "TEST_IMAGES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_CIFAR10_data(TRAIN_IMAGES, VAL_IMAGES, TEST_IMAGES, subtract_mean=False)\n",
    "X_train, y_train = data['X_train'], data['y_train']\n",
    "X_val, y_val = data['X_val'], data['y_val']\n",
    "X_test, y_test = data['X_test'], data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we visualize the dataset to get a better idea of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then subtract all the images with the **mean image** of the training set. This trick is commonly used in Computer Vision community, as it arranges the features into a range that is more manageable for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we convert the sets of images from dimensions of **(N, 3, 32, 32) -> (N, 3072)** where N is the number of images so that each **3x32x32** image is represented by a single vector. This allows us to feed the data into our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we concatenate all the features with ones. This trick is particularly convenient, as in this case, we only need to deal with a single weight matrix $W$ instead of both $W$ and bias $b$ in our training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should get the following output:\n",
    "# (49000, 3073)\n",
    "# (1000, 3073)\n",
    "# (1000, 3073)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Classification Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple helper function for calculation the classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(pred, y_test):\n",
    "    return np.sum(y_test==pred)/len(y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test get_acc\n",
    "print(get_acc(y_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. Support Vector Machines (with SGD) [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will need to implement a **soft margin SVM** for a **multi-class classification problem**. In the soft margin SVM, you will maximize the margin between positive and negative training examples and penalize margin violations using a hinge loss.\n",
    "\n",
    "We will optimize the SVM loss using SGD. This means you must compute the loss function with respect to model weights. You will use this gradient to update the model weights.\n",
    "\n",
    "SVM optimized with SGD has 3 hyperparameters that you can experiment with :\n",
    "- `lr` - the learning rate of your model. This parameter scales by how much the weights are changed according to the calculated gradient update\n",
    "- `n_iter` - the number of training batches you are gonna use for updating your model.\n",
    "- `reg_const` - Hyperparameter to determine the strength of regularization. In this case it is a coefficient on the term which maximizes the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Implementing SVM with SGD [25 pts]\n",
    "\n",
    "Recall that the multiclass SVM loss can be written as:\n",
    "\\begin{align}\n",
    "\\begin{gathered}\n",
    "s=f(x_i, W) \\\\\n",
    "L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, s_j - s_{y_i} + margin) \\right]\n",
    "\\end{gathered}\n",
    "\\end{align}\n",
    "where $s={s_j}$ is the vector of prediction scores of all class, $s_j$ is the prediction score for class $j$, and $y_i$ is the groud truth class label. The margin is usually set to 1. Now, to learn the weights $W$, we can differentiate the function $L_i$ to get the gradient:\n",
    "\\begin{equation}\n",
    "\\begin{gathered}\n",
    "\\nabla_{w_{y_i}} L_i = - \\left( \\sum_{j\\neq y_i} \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + 1 > 0) \\right) x_i\\\\\n",
    "\\end{gathered}\n",
    "\\end{equation}\n",
    "where $\\mathbb{1}$ is the indicator function that is one if the condition inside is true or zero otherwise, and $w_j$ represents the row $j$ in your weight matrix $W$. To implement this, you’d simply count the number of classes that didn’t meet the desired margin 1 (and hence contributed to the loss function) and then the data vector $x_i$ scaled by this number is the gradient. Note that, in the equation above, we are only calculating the gradient with respect to the row of $W$ that corresponds to the correct class $y_i$. For the other rows $j\\neq y_i$, we have:\n",
    "\\begin{equation}\n",
    "\\nabla_{w_j} L_i = \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + 1 > 0) x_i\n",
    "\\end{equation}\n",
    "Now, we have derived the gradient expression above, it is time to turn these equations into the actual running code.\n",
    "We provide a template for implementing your SVM classfier below. You have to fill in the **TODO** part, and train your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "class SVM():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialises Softmax classifier with initializing \n",
    "        weights, alpha(learning rate), number of epochs\n",
    "        and regularization constant.\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.w = np.random.randn(input_size, output_size) * 0.001\n",
    "        \n",
    "    def calc_grad_and_loss(self, X_train, y_train, reg, margin):\n",
    "        \"\"\"\n",
    "          Calculate gradient of the svm hinge loss.\n",
    "          \n",
    "          Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "          of N examples.\n",
    "\n",
    "          Inputs:\n",
    "          - X_train: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "          - y_train: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "            that X[i] has label c, where 0 <= c < C.\n",
    "          - reg: A float number Regularization strength;\n",
    "          - margin: A float number margin for the multi-class SVM loss (usually set to 1);\n",
    "\n",
    "          Returns:\n",
    "          - gradient with respect to weights w; an array of same shape as w;\n",
    "          - multi-class SVM loss;\n",
    "        \"\"\"\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Calculate the loss and gradient for the SVM classifier.                      #\n",
    "        ################################################################################\n",
    "        \n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "        return grad_w, loss\n",
    "        \n",
    "    def train(self, X_train, y_train, lr=0.01, reg_const=0.05, margin=1.0, \n",
    "              n_iters=1500, batch_size=200):\n",
    "        \"\"\"\n",
    "        Train SVM classifier by taking one Stochastic Gradient Descent update\n",
    "        on the input minibatch.\n",
    "\n",
    "        Inputs:\n",
    "        - X_train: A numpy array of shape (N, D) containing training data;\n",
    "        N examples with D dimensions\n",
    "        - y_train: A numpy array of shape (N,) containing training labels;\n",
    "        - lr: A float number learning rate;\n",
    "        - reg_const: A float number Regularization strength;\n",
    "        - margin: A float number margin for the multi-class SVM loss (usually set to 1);\n",
    "        - n_iters: An integer number indicating the number of training iteration;\n",
    "        - batch_size: An integer indicating the number of samples per batch;\n",
    "        Returns:\n",
    "        - loss: Loss for this particular input batch.\n",
    "        \"\"\"\n",
    "        loss_hist = []\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            batch_indices = np.random.choice(len(X_train), batch_size)\n",
    "            X_batch = X_train[batch_indices]\n",
    "            y_batch = y_train[batch_indices]\n",
    "            grad, loss = self.calc_grad_and_loss(X_batch, y_batch, reg_const, margin)\n",
    "            self.w -= lr * grad\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Iter {i+1}/{n_iter} - loss: {loss} ')\n",
    "            loss_hist.append(loss)\n",
    "        return loss_hist\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Use the trained weights of svm classifier to predict labels for\n",
    "        data points.\n",
    "\n",
    "        Inputs:\n",
    "        - X_test: A numpy array of shape (N, D) containing training data; there are N\n",
    "          training samples each of dimension D.\n",
    "\n",
    "        Returns:\n",
    "        - pred: Predicted labels for the data in X_test. pred is a 1-dimensional\n",
    "          array of length N, and each element is an integer giving the predicted\n",
    "          class.\n",
    "        \"\"\"\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Make a prediction based on your classifier weights                           #\n",
    "        ################################################################################\n",
    "        \n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        return pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your SVM\n",
    "Now, train your SVM classifier. You should expect the loss decreases as the training goes on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1500\n",
    "batch_size = 200\n",
    "num_classes = np.max(y_train) + 1\n",
    "num_features = X_train.shape[1]\n",
    "svm = SVM(num_features, num_classes)\n",
    "loss_hist = svm.train(X_train, y_train, lr=1e-7, reg_const=5e3, n_iters=1500,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Loss Curve\n",
    "Plotting the loss curve is a proper etiquette for Machine Learning/Deep Learning practitioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist);\n",
    "plt.ylabel('loss');\n",
    "plt.xlabel('iteration');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy?\n",
    "If the SVM is implemented correctly, you shuold get an training/validation accuracy around 34%-35%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm = svm.predict(X_train)\n",
    "print('The training accuracy is given by : %f' % (get_acc(pred_svm, y_train)))\n",
    "pred_svm = svm.predict(X_val)\n",
    "print('The validation accuracy is given by : %f' % (get_acc(pred_svm, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2. Improve your SVM [5 pts]\n",
    "Now, try to improve the accuracy of your classifier. You can perform hyperparameter search with cross-validation, or conduct some extra preprocessing on the data. **Note that you should never use the `X_test` and `y_test` for tuning your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can you improve it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test SVM\n",
    "Evaluate the testing accuracy of your SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm = svm.predict(X_test)\n",
    "print('The testing accuracy is given by : %f' % (get_acc(pred_svm, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Visualize the weights of your best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class.\n",
    "# Hint: reshape your weights (w/o bias) to (32, 32, 3, 10), then use min-max normalization and rescale them back to 0-255.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# plot for each class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe what your visualized SVM weights look like, and offer a brief explanation for why they look the way that they do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Softmax Classifier (with SGD) [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, you will train a Softmax classifier. This classifier consists of a linear function of the input data followed by a softmax function which outputs a vector of dimension C (number of classes) for each data point. Each entry of the softmax output vector corresponds to a confidence in one of the C classes, and like a probability distribution, the entries of the output vector sum to 1. \n",
    "\n",
    "## Section 2.1. Implement the Softmax Classifier [25 pts]\n",
    "Recall that, from the lecture, we can write down the loss function as:\n",
    "\\begin{align}\n",
    "\\begin{gathered}\n",
    "s=f(x_i, W) \\\\\n",
    "L_i = -\\log\\left({\\frac{e^{s_{y_i}}} {\\sum_j e^{s_j}} }\\right)\n",
    "\\end{gathered}\n",
    "\\end{align}\n",
    "\n",
    "And, through some calculations, we can obtain the expression for the gradient:\n",
    "\\begin{align}\n",
    "\\nabla_{w_{l}} L_i = \\left(\\frac{e^{w_l^Tx_i}} {\\sum_j e^{w_j^Tx_i}} - \\mathbb{1}\\left(y_i=l\\right)\\right) x_i\\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax classifier also has 3 hyperparameters that you can experiment with :\n",
    "- `lr` - the learning rate of your model. This parameter scales by how much the weights are changed according to the calculated gradient update\n",
    "- `n_iter` - the number of training batches you are gonna use for updating your model.\n",
    "- `reg_const` - Hyperparameter to determine the strength of regularization. In this case it is a coefficient on the term which maximizes the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as the previous section, we provide a template for you to implement your classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " class Softmax_Classfier():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialises Softmax classifier with initializing \n",
    "        weights, alpha(learning rate), number of epochs\n",
    "        and regularization constant.\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.w = np.random.randn(input_size, output_size) * 0.001\n",
    "    \n",
    "    def softmax(self, logits):\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Implement the softmax function here.                                         #\n",
    "        ################################################################################\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        return probs\n",
    "    \n",
    "    def calc_grad_and_loss(self, X_train, y_train, reg_const):\n",
    "        \"\"\"\n",
    "          Calculate gradient of the svm hinge loss.\n",
    "          \n",
    "          Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "          of N examples.\n",
    "\n",
    "          Inputs:\n",
    "          - X_train: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "          - y_train: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "            that X[i] has label c, where 0 <= c < C.\n",
    "          - reg: A float number Regularization strength;\n",
    "\n",
    "          Returns:\n",
    "          - gradient with respect to weights w; an array of same shape as w;\n",
    "          - multi-class SVM loss;\n",
    "         \"\"\" \n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Calculate the loss and gradient for the SVM classifier.                      #\n",
    "        ################################################################################\n",
    "        \n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        return grad_w, loss\n",
    "        \n",
    "    def train(self, X_train, y_train, lr=0.01, reg_const=0.05, \n",
    "              n_iters=1500, batch_size=200):\n",
    "        \"\"\"\n",
    "        Train softmax classifier by taking one Stochastic Gradient Descent update\n",
    "        on the input minibatch.\n",
    "\n",
    "        Inputs:\n",
    "        - X_train: A numpy array of shape (N, D) containing training data;\n",
    "        N examples with D dimensions\n",
    "        - y_train: A numpy array of shape (N,) containing training labels;\n",
    "        - lr: A float number learning rate;\n",
    "        - reg_const: A float number Regularization strength;\n",
    "        - n_iters: An integer number indicating the number of training iteration;\n",
    "        - batch_size: An integer indicating the number of samples per batch;\n",
    "        Returns:\n",
    "        - loss: Loss for this particular input batch.\n",
    "        \"\"\"\n",
    "        loss_hist = []\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            batch_indices = np.random.choice(len(X_train), batch_size)\n",
    "            X_batch = X_train[batch_indices]\n",
    "            y_batch = y_train[batch_indices]\n",
    "            grad, loss = self.calc_grad_and_loss(X_batch, y_batch, reg_const)\n",
    "            self.w -= lr * grad\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Iter {i+1}/{n_iter} - loss: {loss} ')\n",
    "            loss_hist.append(loss)\n",
    "        return loss_hist\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Use the trained weights of svm classifier to predict labels for\n",
    "        data points.\n",
    "\n",
    "        Inputs:\n",
    "        - X_test: A numpy array of shape (N, D) containing training data; there are N\n",
    "          training samples each of dimension D.\n",
    "\n",
    "        Returns:\n",
    "        - pred: Predicted labels for the data in X_test. pred is a 1-dimensional\n",
    "          array of length N, and each element is an integer giving the predicted\n",
    "          class.\n",
    "        \"\"\"\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Make a prediction based on your classifier weights                           #\n",
    "        ################################################################################\n",
    "        \n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1500\n",
    "batch_size = 200\n",
    "num_classes = np.max(y_train) + 1\n",
    "num_features = X_train.shape[1]\n",
    "softmax_classifier = Softmax_Classfier(num_features, num_classes)\n",
    "loss_hist = softmax_classifier.train(X_train, y_train, lr=1e-7, reg_const=5e3, n_iters=1500, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, plot the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist);\n",
    "plt.ylabel('loss');\n",
    "plt.xlabel('iteration');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy?\n",
    "If the Softmax Classifier is implemented correctly, you shuold get an accuracy around 29%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_softmax = softmax_classifier.predict(X_train)\n",
    "print('The training accuracy is given by : %f' % (get_acc(pred_softmax, y_train)))\n",
    "pred_softmax = softmax_classifier.predict(X_val)\n",
    "print('The validation accuracy is given by : %f' % (get_acc(pred_softmax, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1. Improve your Softmax Classifier [5 pts]\n",
    "Again, try to improve the accuracy of your classifier. You can perform hyperparameter search with cross-validation, or conduct some extra preprocessing on the data. **Note that you should never use the `X_test` and `y_test` for tuning your model.** In addition, <span style=\"color:red\">**We expect to see an accuracy around 34% after the improvement.**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the accuracy to above 34%! \n",
    "# For hyperparameter search, you can try different combinations of (reg_const, margin, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, evaluate the testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_softmax = softmax_classifier.predict(X_test)\n",
    "print('The testing accuracy is given by : %f' % (get_acc(pred_softmax, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Visualize the weights of your best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class.\n",
    "# Hint: reshape your weights (no bias) to (32, 32, 3, 10), and then rescale them back to 0-255 using min-max normalization.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# plot for each class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe what your visualized weights look like compared to SVM weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3. Multilayer Perceptron (MLP)\n",
    "In this section, we present you a modified version of simple neural network from Professor Vicente Ordonez at the University of Virginia. You job is to read through the implementation (optional, but highly recommended), and try the MLP classifier. For additional resources on understanding forward and backpropagation, you can check the materials here: http://cs231n.github.io/optimization-2/\n",
    "\n",
    "Consider a single-layer supervised neural network that has 4 inputs and 3 outputs. First, let's review the skeleton of a single linear layer neural network. The inputs of the network are the variables $x_1, x_2, x_3, x_4$, or the input vector $\\mathbf{x}=[x_1, x_2, x_3, x_4]$, the outputs of the network are $\\widehat{y}_1,\\widehat{y}_2,\\widehat{y}_3$, or the output vector $\\widehat{\\mathbf{y}}=[$$\\widehat{y}$$_1,\\widehat{y}_2,\\widehat{y}_3]$:\n",
    "\n",
    "<img src=\"fig/1_layer_fig.png\" width=\"450\"/>\n",
    "\n",
    "The given $j$-th output $\\widehat{y}_j$ of this single linear layer + activation function is computed as follows:\n",
    "\n",
    "$$\\widehat{y}_j= \\text{sigmoid}(w_{1j}x_1 + w_{2j}x_2 + w_{3j}x_3 + w_{4j}x_4 + b_j) = \\text{sigmoid}\\Big(\\sum_{i=1}^{i=4}w_{ij}x_{i} + b_j\\Big)$$\n",
    "\n",
    "In matrix notation, this would be: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "  \\widehat{y}_{1} \\\\ \n",
    "  \\widehat{y}_{2} \\\\\n",
    "  \\widehat{y}_{3} \n",
    "\\end{bmatrix}^T=\\mathbf{Sigmoid}\\Bigg(\n",
    "\\begin{bmatrix}\n",
    "  x_{1} \\\\\n",
    "  x_{2} \\\\\n",
    "  x_{3} \\\\\n",
    "  x_{4}\n",
    "\\end{bmatrix}^T\n",
    "\\begin{bmatrix}\n",
    "  w_{1,1} & w_{1,2} & w_{1,3}\\\\\n",
    "  w_{2,1} & w_{3,2} & w_{2,3}\\\\\n",
    "  w_{3,1} & w_{3,2} & w_{3,3}\\\\\n",
    "  w_{4,1} & w_{4,2} & w_{4,3}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "  b_{1} \\\\\n",
    "  b_{2} \\\\\n",
    "  b_{3}\n",
    "\\end{bmatrix}^T\\Bigg)\n",
    "\\end{equation}\n",
    "\n",
    "or more compactly:\n",
    "\n",
    "\\begin{equation}\n",
    "\\widehat{\\mathbf{y}}^T = \\mathbf{Sigmoid}(\\mathbf{x}^T \\cdot \\mathbf{W} + \\mathbf{b}^T)\n",
    "\\end{equation}\n",
    "\n",
    "The element-wise sigmoid function is: $\\mathbf{Sigmoid}(\\mathbf{x}) = 1 \\;/\\; (1 + exp(-\\mathbf{x}))$, or alternatively: $\\mathbf{Sigmoid}(\\mathbf{x}) = exp(\\mathbf{x})\\;/\\;(1 + exp(\\mathbf{x}))$. Here the sigmoid is separated logically into an activation layer $\\sigma(x)$ and a linear layer $\\text{linear}(3,4)$ as illustrated in figure. \n",
    "\n",
    "Training these weights $\\mathbf{W}$ and biases $\\mathbf{b}$ requires having many training pairs $(\\widehat{\\mathbf{y}}^{(m)}, \\mathbf{x}^{(m)})$. The inputs $\\mathbf{x}$ can be the pixels of an image, indices of words, the entries in a database, and the outputs $\\widehat{\\mathbf{y}}$ can also be literally anything, including a number indicating a category, a set of numbers indicating the indices of words composing a sentence, an output image itself, etc.\n",
    "\n",
    "## 3.1. Forward-propagation\n",
    "\n",
    "Computing the outputs $\\widehat{\\mathbf{y}}$ from the inputs $\\mathbf{x}$ in this network composed of a single linear layer, and a sigmoid layer, is called forward-propagation. Below is the code that implements these two operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "\n",
    "class nn_Sigmoid:\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class nn_Linear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialized with random numbers from a gaussian N(0, 0.001)\n",
    "        self.weight = np.matlib.randn(input_dim, output_dim) * 0.001\n",
    "        self.bias = np.matlib.randn((1, output_dim)) * 0.001\n",
    "        \n",
    "    # y = Wx + b\n",
    "    def forward(self, x):\n",
    "        return np.dot(x, self.weight) + self.bias\n",
    "    \n",
    "    def getParameters(self):\n",
    "        return [self.weight, self.bias]\n",
    "\n",
    "# Let's test the composition of the two functions (forward-propagation in the neural network).\n",
    "x1 = np.array([[1, 2, 2, 3]])\n",
    "y_hat1 = nn_Sigmoid().forward(nn_Linear(4, 3).forward(x1))\n",
    "print('x[1] = '+ str(x1))\n",
    "print('y_hat[1] = ' + str(y_hat1) + '\\n')\n",
    "\n",
    "# Let's test the composition of the two functions (forward-propagation in the neural network).\n",
    "x2 = np.array([[4, 5, 2, 1]])\n",
    "y_hat2 = nn_Sigmoid().forward(nn_Linear(4, 3).forward(x2))\n",
    "print('x[2] = '+ str(x2))\n",
    "print('y_hat[2] = ' + str(y_hat2) + '\\n')\n",
    "\n",
    "# We can also compute both at once, which could be more efficient since it requires a single matrix multiplication.\n",
    "x = np.concatenate((x1, x2), axis = 0)\n",
    "y_hat = nn_Sigmoid().forward(nn_Linear(4, 3).forward(x))\n",
    "print('x = ' + str(x))\n",
    "print('y_hat = ' + str(y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Loss functions.\n",
    "\n",
    "After computing the output predictions $\\widehat{\\mathbf{y}}$ it is necessary to compare these against the true values of $\\mathbf{y}$. Let's call these true, correct, or desired values $\\mathbf{y}$. Typically, a simple loss or cost function is used to measure the degree by which the prediction $\\widehat{\\mathbf{y}}$ is wrong with respect to $\\mathbf{y}$. A common loss function for regression is the sum of squared differences between the prediction and its true value. Assuming a prediction $\\widehat{\\mathbf{y}}^{(d)}$ for our training sample $\\mathbf{x}^{(d)}$ with true value $\\mathbf{y}^{(d)}$, then the loss can be computed as:\n",
    "\n",
    "$$loss(\\widehat{\\mathbf{y}}^{(d)}, \\mathbf{y}^{(d)}) = (\\widehat{y}^{(d)}_1 - y^{(d)}_1)^2 + (\\widehat{y}^{(d)}_2 - y^{(d)}_2)^2 + (\\widehat{y}^{(d)}_3 - y^{(d)}_3)^2 = \\sum_{j=1}^{j=3}(\\widehat{y}^{(d)}_j - y^{(d)}_j)^2$$\n",
    "\n",
    "The goal is to modify the parameters [$\\mathbf{W}, \\mathbf{b}$] in the Linear layer so that the value of $loss(\\widehat{\\mathbf{y}}^{(d)}, \\mathbf{y}^{(d)})$ becomes as small as possible for all training samples in a set $D=\\{(\\mathbf{x}^{(d)},\\mathbf{y}^{(d)})\\}$. This would in turn ensure that predictions $\\widehat{\\mathbf{y}}$ are as similar as possible to the true values $\\mathbf{y}$. To achieve this we need to minimize the following function:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{W}, \\mathbf{b}) = \\sum_{d=1}^{d=|D|} loss(\\widehat{\\mathbf{y}}^{(d)}, \\mathbf{y}^{(d)})$$\n",
    "\n",
    "The only two variables for our model in the function $\\mathcal{L}(\\mathbf{W}, \\mathbf{b})$ are $\\mathbf{W}$ and $\\mathbf{b}$, this is because the training dataset $D$ is fixed. Finding the values of $\\mathbf{W}$ and $\\mathbf{b}$ that minimize the the loss, particularly for complex functions, is typically done using gradient based optimization, like Stochastic Gradient Descent (SGD). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_MSECriterion:  # MSE = mean squared error.\n",
    "    def forward(self, predictions, labels):\n",
    "        return np.sum(np.square(predictions - labels))\n",
    "\n",
    "# Let's test the loss function.\n",
    "y_hat = np.array([[0.23, 0.25, 0.33], [0.23, 0.25, 0.33], [0.23, 0.25, 0.33], [0.23, 0.25, 0.33]])\n",
    "y_true = np.array([[0.25, 0.25, 0.25], [0.33, 0.33, 0.33], [0.77, 0.77, 0.77], [0.80, 0.80, 0.80]])\n",
    "\n",
    "nn_MSECriterion().forward(y_hat, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Backward-propagation (Backpropagation)\n",
    "\n",
    "As we discussed in class, backpropagation is just applying the chain-rule in calculus to compute the derivative of a function which is the composition of many functions (this is essentially definition of the neural network). \n",
    "\n",
    "Below is the implementation of required derivative computations for our simple network. You are highly advised to derive the derivatives implemented here to make sure you understand how one arrives at them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is referred above as f(u).\n",
    "\n",
    "class nn_MSECriterion:\n",
    "    def forward(self, predictions, labels):\n",
    "        return np.sum(np.square(predictions - labels))\n",
    "        \n",
    "    def backward(self, predictions, labels):\n",
    "        num_samples = labels.shape[0]\n",
    "        return 2 * (predictions - labels)\n",
    "    \n",
    "# This is referred above as g(v).\n",
    "class nn_Sigmoid:\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        # It is usually a good idea to use gv from the forward pass and not recompute it again here.\n",
    "        gv = 1 / (1 + np.exp(-x))\n",
    "        return np.multiply(np.multiply(gv, (1 - gv)), gradOutput)\n",
    "\n",
    "# This is referred above as h(W, b)\n",
    "class nn_Linear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialized with random numbers from a gaussian N(0, 0.001)\n",
    "        self.weight = np.matlib.randn(input_dim, output_dim) * 0.001\n",
    "        self.bias = np.matlib.randn((1, output_dim)) * 0.001\n",
    "        self.gradWeight = np.zeros_like(self.weight)\n",
    "        self.gradBias = np.zeros_like(self.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return np.dot(x, self.weight) + self.bias\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        # dL/dw = dh/dw * dL/dv\n",
    "        self.gradWeight = np.dot(x.T, gradOutput)\n",
    "        # dL/db = dh/db * dL/dv\n",
    "        #self.gradBias = np.copy(gradOutput)\n",
    "        self.gradBias = np.copy(np.sum(gradOutput, axis=0))\n",
    "        # return dL/dx = dh/dx * dL/dv\n",
    "        return np.dot(gradOutput, self.weight.T)\n",
    "    \n",
    "    def getParameters(self):\n",
    "        params = [self.weight, self.bias]\n",
    "        gradParams = [self.gradWeight, self.gradBias]\n",
    "        return params, gradParams\n",
    "    \n",
    "# Let's test some dummy inputs for a full pass of forward and backward propagation.\n",
    "x1 = np.array([[1, 2, 2, 3]])\n",
    "y1 = np.array([[0.25, 0.25, 0.25]])\n",
    "\n",
    "# Define the operations.\n",
    "linear = nn_Linear(4, 3)  # h(W, b)\n",
    "sigmoid = nn_Sigmoid()  # g(v)\n",
    "loss = nn_MSECriterion()  # f(u)\n",
    "\n",
    "# Forward-propagation.\n",
    "lin = linear.forward(x1)\n",
    "y_hat = sigmoid.forward(lin)\n",
    "loss_val = loss.forward(y_hat, y1) # Loss function.\n",
    "\n",
    "# Backward-propagation.\n",
    "dy_hat = loss.backward(y_hat, y1)\n",
    "dlin = sigmoid.backward(lin, dy_hat)\n",
    "dx1 = linear.backward(x1, dlin)\n",
    "\n",
    "print('\\n num_samples = ' + str(y1.shape[0]))\n",
    "\n",
    "# Show parameters of the linear layer.\n",
    "print('\\nW = ' + str(linear.weight))\n",
    "print('B = ' + str(linear.bias))\n",
    "\n",
    "# Show the intermediate outputs in the forward pass.\n",
    "print('\\nx1    = '+ str(x1))\n",
    "print('lin   = ' + str(lin))\n",
    "print('y_hat = ' + str(y_hat))\n",
    "\n",
    "print('\\nloss = ' + str(loss_val))\n",
    "\n",
    "# Show the intermediate gradients with respect to inputs in the backward pass.\n",
    "print('\\ndy_hat = ' + str(dy_hat))\n",
    "print('dlin   = ' + str(dlin))\n",
    "print('dx1    = ' + str(dx1))\n",
    "\n",
    "# Show the gradients with respect to parameters.\n",
    "print('\\ndW = ' + str(linear.gradWeight))\n",
    "print('dB = ' + str(linear.gradBias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Gradient checking \n",
    "\n",
    "The gradients can also be computed with numerical approximation using the definition of derivatives. Let a single input pair $(\\mathbf{x}, \\mathbf{y})$ be the input, for each entry $w_{ij}$ in the weight matrix $\\mathbf{W}$, the partial derivatives can be computed as follows:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}(\\mathbf{W},\\mathbf{b})}{\\partial w_{ij}} = \\frac{\\mathcal{L}(\\mathbf{W} + \\mathbf{E}_{ij},b) - \\mathcal{L}(\\mathbf{W} - \\mathbf{E}_{ij}, b)}{2\\epsilon}, $$\n",
    "\n",
    "where $\\mathbf{E}_{ij}$ is a matrix that has $\\epsilon$ in its $(i,j)$ entry and zeros everywhere else. Intuitively this gradient tells how would the value of the loss changes if we change a particular weight $w_{ij}$ by an $\\epsilon$ amount. We can do the same to compute derivatives with respect to the bias parameters $b_i$. Below is the code that checks for a given input $(\\mathbf{x}, \\mathbf{y})$, the gradients for the matrix $\\mathbf{W}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will compute derivatives with respect to a single data pair (x,y)\n",
    "x = np.array([[2.34, 3.8, 34.44, 5.33]])\n",
    "y = np.array([[3.2, 4.2, 5.3]])\n",
    "\n",
    "# Define the operations.\n",
    "linear = nn_Linear(4, 3)\n",
    "sigmoid = nn_Sigmoid()\n",
    "criterion = nn_MSECriterion()\n",
    "\n",
    "# Forward-propagation.\n",
    "a0 = linear.forward(x)\n",
    "a1 = sigmoid.forward(a0)\n",
    "loss = criterion.forward(a1, y) # Loss function.\n",
    "\n",
    "# Backward-propagation.\n",
    "da1 = criterion.backward(a1, y)\n",
    "da0 = sigmoid.backward(a0, da1)\n",
    "dx = linear.backward(x, da0)\n",
    "\n",
    "gradWeight = linear.gradWeight\n",
    "gradBias = linear.gradBias\n",
    "\n",
    "approxGradWeight = np.zeros_like(linear.weight)\n",
    "approxGradBias = np.zeros_like(linear.bias)\n",
    "\n",
    "# We will verify here that gradWeights are correct and leave it as an excercise\n",
    "# to verify the gradBias.\n",
    "epsilon = 0.0001\n",
    "for i in range(0, linear.weight.shape[0]):\n",
    "    for j in range(0, linear.weight.shape[1]):\n",
    "        # Compute f(w)\n",
    "        fw = criterion.forward(sigmoid.forward(linear.forward(x)), y) # Loss function.\n",
    "        # Compute f(w + eps)\n",
    "        shifted_weight = np.copy(linear.weight)\n",
    "        shifted_weight[i, j] = shifted_weight[i, j] + epsilon\n",
    "        shifted_linear = nn_Linear(4, 3)\n",
    "        shifted_linear.bias = linear.bias\n",
    "        shifted_linear.weight = shifted_weight\n",
    "        fw_epsilon = criterion.forward(sigmoid.forward(shifted_linear.forward(x)), y) # Loss function\n",
    "        # Compute (f(w + eps) - f(w)) / eps\n",
    "        approxGradWeight[i, j] = (fw_epsilon - fw) / epsilon\n",
    "\n",
    "# These two outputs should be similar up to some precision.\n",
    "print('gradWeight: ' + str(gradWeight))\n",
    "print('\\napproxGradWeight: ' + str(approxGradWeight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.4. Playing with the Simple NN [30 pts]\n",
    "Now, we provide a simple NN model that has an training accuracy around 46%. You job here is to first run and check the simple NN model, and the experiments it with:\n",
    "- Try using 3 different numbers of layers for the simple NN, and plot the accuracy as a function of the number of layers. \n",
    "- Try 4 different numbers of hidden state size, and plot the accuracy as a function of the number you have tried.\n",
    "- Try different activation function (Sigmoid, ReLU)\n",
    "- Briefly describe what you've observed in the above experiments\n",
    "\n",
    "Note that you are free to create your own Simple_NN class for your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_CECriterionWithLogits:\n",
    "    def forward(self, logits, labels):\n",
    "        y_indices = np.arange(len(labels))\n",
    "        self.probs = softmax(logits)\n",
    "        correct_class_prob = self.probs[y_indices, labels].reshape(-1, 1)\n",
    "        loss = np.sum(-np.log(correct_class_prob)) / len(correct_class_prob)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, labels):\n",
    "        y_indices = np.arange(len(labels))\n",
    "        gradOutput = self.probs\n",
    "        gradOutput[y_indices, labels] -= 1\n",
    "        return gradOutput / len(labels)\n",
    "    \n",
    "class nn_ReLU:\n",
    "    def forward(self, x):\n",
    "        # Forward pass.\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        # Backward pass\n",
    "        return np.where(x < 0, 0, np.multiply(x, gradOutput))\n",
    "    \n",
    "def softmax(x):\n",
    "    exp = np.exp(x - x.max()) # stability trick\n",
    "    return (exp / np.sum(exp, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_NN(object):\n",
    "    def __init__(self, input_size, output_size, hidden_state_size=64):\n",
    "        self.linear1 = nn_Linear(input_size, hidden_state_size)\n",
    "        self.activation = nn_ReLU()\n",
    "        self.linear2 = nn_Linear(hidden_state_size, output_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.x1 = self.linear1.forward(self.X)\n",
    "        self.a1 = self.activation.forward(self.x1)\n",
    "        logits = self.linear2.forward(self.a1)\n",
    "        return logits\n",
    "    \n",
    "    def backward(self, gradOutput):\n",
    "        dx2 = self.linear2.backward(self.a1, gradOutput)\n",
    "        da1 = self.activation.backward(self.x1, dx2)\n",
    "        self.linear1.backward(self.X, da1)\n",
    "    \n",
    "    def update(self, lr):\n",
    "        self.linear1.weight = self.linear1.weight - lr * (self.linear1.gradWeight)\n",
    "        self.linear1.bias = self.linear1.bias - lr * (self.linear1.gradBias)\n",
    "        self.linear2.weight = self.linear2.weight - lr * (self.linear2.gradWeight)\n",
    "        self.linear2.bias = self.linear2.bias - lr * (self.linear2.bias)\n",
    "        \n",
    "    def train(self, X_train, y_train, loss_criterion=nn_CECriterionWithLogits(), \n",
    "              lr=1e-5, n_iter=1500, batch_size=200):\n",
    "        loss_hist = []\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            batch_indices = np.random.choice(len(X_train), batch_size)\n",
    "            X_batch = X_train[batch_indices]\n",
    "            y_batch = y_train[batch_indices]\n",
    "            \n",
    "            logits = self.forward(X_batch)\n",
    "            loss = loss_criterion.forward(logits, y_batch)\n",
    "            loss_grad = loss_criterion.backward(y_batch)\n",
    "            self.backward(loss_grad)\n",
    "            self.update(lr)\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Iter {i+1}/{n_iter} - loss: {loss} ')\n",
    "            loss_hist.append(loss)\n",
    "        return loss_hist\n",
    "    def predict(self, X):\n",
    "        return np.asarray(np.argmax(softmax(self.forward(X)), axis=1)).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the bias is explicitly handled in the MLP code, stripped off the ones we have concatenated to the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:, :3072]\n",
    "X_val = X_val[:, :3072]\n",
    "X_test = X_test[:, :3072]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the simple NN to check the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Simple_NN(X_train.shape[1], np.max(y_train)+1, hidden_state_size=64)\n",
    "n_iter = 1500\n",
    "batch_size = 200\n",
    "loss_hist = model.train(X_train, y_train, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist);\n",
    "plt.ylabel('loss');\n",
    "plt.xlabel('iteration');\n",
    "predict = model.predict(X_train)\n",
    "print('The training accuracy is given by : %f' % (get_acc(predict, y_train)))\n",
    "predict = model.predict(X_val)\n",
    "print('The training accuracy is given by : %f' % (get_acc(predict, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.4.1. Different numbers of layers [12 pts]\n",
    "Try building 3 simple NN with different numbers of layers. Plot the training/validation/test accuracy as a function of the numbers of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.4.2. Different numbers of hidden state size [7 pts]\n",
    "Try 4 different numbers of hidden state size, and plot the accuracy as a function of the number you have tried. We recommend you to test numbers like 8, 16, 32, 64, 128, 256. Plot the training/validation/test accuracy as a function of the hidden state size your picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.4.3. Different activation functions [7 pts]\n",
    "Try both ReLU and Sigmoid activation, and report the training/validation/test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.4.4. What did you see? [4 pts]\n",
    "Desribe what you see in the experiments above in 5-6 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
