{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE-6524 / CS-6524 Deep Learning\n",
    "# Assignment 2 [80 pts]\n",
    "\n",
    "In this assignment, **you need to complete the following sections**:\n",
    "1. PyTorch Basics\n",
    "    - Toy example with PyTorch\n",
    "2. Image Classification with PyTorch\n",
    "    - Implement a simple MLP network for image classification\n",
    "    - Implement a convolutional network for image classification\n",
    "    - Experiment with different numbers of layers and optimizers\n",
    "    - Push the performance of your CNN\n",
    "\n",
    "This assignment is inspired and adopted from the official PyTorch tutorial.\n",
    "## Submission guideline for the coding part (Jupyter Notebook)\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook\n",
    "2. Please make sure to have entered your Virginia Tech PID below\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of cells)\n",
    "4. Select Cell -> Run All. This will run all the cells in order\n",
    "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX\n",
    "6. Look at the PDF file and make sure all your solutions are displayed correctly there \n",
    "7. Zip all the files along with this notebook (Please don't include the data). Name it as Assignment_2_Code_[YOUR PID NUMBER].zip\n",
    "8. Name your PDF file as Assignment_2_NB_[YOUR PID NUMBER].pdf\n",
    "9. **<span style=\"color:blue\"> Submit your zipped file and the PDF SEPARATELY**</span>\n",
    "\n",
    "Note: if facing issues with step 5 refer: https://pypi.org/project/notebook-as-pdf/\n",
    "\n",
    "## Submission guideline for the coding part (Google Colab)\n",
    "\n",
    "1. Click the Save button at the top of the Notebook\n",
    "2. Please make sure to have entered your Virginia Tech PID below\n",
    "3. Follow last two cells in this notebook for guidelines to download pdf file of this notebook\n",
    "4. Look at the PDF file and make sure all your solutions are displayed correctly there \n",
    "5. Zip all the files along with this notebook (Please don't include the data). Name it as Assignment_2_Code_[YOUR PID NUMBER].zip\n",
    "6. Name your PDF file as Assignment_2_NB_[YOUR PID NUMBER].pdf\n",
    "7. **<span style=\"color:blue\"> Submit your zipped file and the PDF SEPARATELY**</span>\n",
    "\n",
    "\n",
    "**While you are encouraged to discuss with your peers, <span style=\"color:blue\">all work submitted is expected to be your own.</span> <span style=\"color:red\">If you use any information from other resources (e.g. online materials), you are required to cite it below you VT PID. Any violation will result in a 0 mark for the assignment.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please Write Your VT PID Here: \n",
    "### Reference (if any):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you would need to use **Python 3.6+** along with the following packages:\n",
    "```\n",
    "1. pytorch 1.2\n",
    "2. torchvision\n",
    "3. numpy\n",
    "4. matplotlib\n",
    "```\n",
    "To install pytorch, please follow the instructions on the [Official website](https://pytorch.org/). In addition, the [official document](https://pytorch.org/docs/stable/) could be very helpful when you want to find certain functionalities. \n",
    "\n",
    "You can also consider to use Google Colab, where PyTorch has been installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. PyTorch Basics [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply put, PyTorch is a **Tensor** library like Numpy. These two libraries similarly provide useful and efficient APIs for you to deal with your tensor data. What really differentiate PyTorch from Numpy are the following two features:\n",
    "1. Numerical operations that can **run on GPUs** (more than 10x speedup)\n",
    "2. Automatic differentiation for building and training neural networks\n",
    "\n",
    "In this section, we will walk through some simple example, and see how the automatic differentiation functionality can make your life much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To select GPU in Google Colab:\n",
    "- go to **Edit -> Notebook settings -> Hardware accelerator -> GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # import pytorch.\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "#print(torch.cuda.get_device_name(0)) # Check GPU Device name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Automatic Differentiation\n",
    "Gradient descent is the driving force of the deep learning field. In the lectures and assignment 1, we learned how to derive the gradient for a given function, and implement methods for calculating and performing gradient descents. We also see how we can manually implement the backward and forward functions for the simple NN example. While implementing these functions may not be a big deal for a small network, it may get very nasty when we want to build something with tens of hundreds of layers.\n",
    "\n",
    "In PyTorch (as well as other major deep learning libraries), we can use autograd ([automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)) to handle the tedious computation of backward passes. When doing forward passes with autograd, we are essentially defining a **computational graph**, while the nodes in the graph are **tensors**, the edges are the functions that produce output tensors (e.g. ReLU, Linear, Convolutional Layer) given the input tensors. To do backpropagation, we can simply backtrack through this graph to compute gradients. \n",
    "\n",
    "This may sound a little bit abstract, so let's take a look at the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 10.\n",
    "\n",
    "# create a matrix of size 2x2. Each with value draws from standard normal distribution.\n",
    "x = torch.randn(2, 2, requires_grad=True) \n",
    "y = torch.randn(2, 2, requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "b = a.sum()\n",
    "loss = b - target\n",
    "\n",
    "# print out each tensor:\n",
    "print(x)\n",
    "print(y)\n",
    "print(a)\n",
    "print(b)\n",
    "print(loss)\n",
    "\n",
    "print(\"-----gradient-----\")\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we have seen a few things:\n",
    "1. `requires_grad` flag: If false, we can safely exclude this tensor (and its subgraph) from gradient computation and therefore increase efficiency.\n",
    "2. `grad_fn`: we can see that once an operation is done to a tensor, the output tensor is bound to a backward function associated to the operation. In this case, we have Add, Sum, and Sub.\n",
    "\n",
    "However, even if we set `requires_grad=True`, we still don't have gradient for `x` and `y`. This is because that we haven't performed the backpropagation yet. So let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform backpropagation from this \"node\"\n",
    "loss.backward()\n",
    "print('-----gradient-----')\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, seems like we can perform gradient descent without writing backwards function! Now, let's see a simple toy example on how we can fit some weights `w1` and `w2` with random input `x` and target `y`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(f'iteration {t}: {loss.item()}')\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.  (because we don't need the gradient for the operation \n",
    "    # learning_rate * w1.grad)\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. `nn` Module\n",
    "Computational graphs and autograd are a very powerful paradigm for defining complex operators and automatically taking derivatives; however for large neural networks raw autograd can be a bit too low-level.\n",
    "\n",
    "When building neural networks we frequently think of arranging the computation into layers, some of which have learnable parameters which will be optimized during learning.\n",
    "\n",
    "In PyTorch, the nn package serves this purpose. The nn package defines a set of Modules, which are roughly equivalent to neural network layers. A Module receives input Tensors and computes output Tensors, but may also hold internal state such as Tensors containing learnable parameters. The nn package also defines a set of useful loss functions that are commonly used when training neural networks.\n",
    "\n",
    "Now, let's see how our simple NN could be implemented using the nn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_in, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(f'iteration {t}: {loss.item()}')\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have been updating the model parameters manually with `torch.no_grad()`. However, if we want to use optimization algorithms other than SGD, it might get a bit nasty to do it manually. Instead of manually doing this, we can use `optim` pacakge to help optimize our model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_in, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. \n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(f'iteration {t}: {loss.item()}')\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you will want to specify models that are more complex than a sequence of existing Modules; for these cases you can define your own Modules by subclassing nn.Module and defining a forward which receives input Tensors and produces output Tensors using other modules or other autograd operations on Tensors.\n",
    "\n",
    "For example, we can implement our 2-layer simple NN as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H)\n",
    "        self.linear2 = nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(f'iteration {t}: {loss.item()}')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Warm-up: Two-moon datasets [10 pts]\n",
    "Now, let's use PyTorch to solve some synthetic datasets. In previous assignment, we have to write some codes to create training batches. Again, this can also be done with PyTorch `DataLoader`. The `DataLoader` utilizes parallel workers to read and prepare batches for you, which can greatly speedup the code when your time bottleneck is on file I/O.\n",
    "\n",
    "Here, we show a simple example that can create a dataloader from numpy data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Google Colab (Skip for Jupyter Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find path to your data folder in drive and enter for \"path_to_dataset\"\n",
    "path_to_dataset = '/content/drive/My Drive/DL_Fall_2020/Assignment_2/data'\n",
    "# For Jupyter notebook give path from your local PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train = np.loadtxt(path_to_dataset + '/X1_train.csv', delimiter=',')\n",
    "X_test = np.loadtxt(path_to_dataset + '/X1_test.csv', delimiter=',')\n",
    "y_train = np.loadtxt(path_to_dataset + '/y1_train.csv', delimiter=',')\n",
    "y_test = np.loadtxt(path_to_dataset + '/y1_test.csv', delimiter=',')\n",
    "\n",
    "# Plot it to see why is it called two-moon dataset\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a PyTorch `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "batch_size = 64 # mini-batch size\n",
    "num_workers = 4 # how many parallel workers are we gonna use for reading data\n",
    "shuffle = True # shuffle the dataset\n",
    "\n",
    "# Convert numpy array import torch tensor\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train.reshape(-1, 1))\n",
    "y_test = torch.LongTensor(y_test.reshape(-1, 1))\n",
    "\n",
    "# First, create a dataset from torch tensor. A dataset define how to read data\n",
    "# and process data for creating mini-batches.\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          num_workers=num_workers, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we provide a simple example on how to train your model with this dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5 # an epoch means looping through all the data in the datasets\n",
    "lr = 1e-1\n",
    "\n",
    "# create a simple model that is probably not gonna work well\n",
    "model = nn.Linear(X_train.size(1), 1)\n",
    "optim = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for e in range(epoch):\n",
    "    loss_epoch = 0\n",
    "    # loop through train loader to get x and y\n",
    "    for x, y in train_loader:\n",
    "        optim.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        # !!WARNING!!\n",
    "        # THIS IS A CLASSIFICATION TASK, SO YOU SHOULD NOT\n",
    "        # USE THIS LOSS FUNCTION. \n",
    "        loss = (y_pred - y.float()).abs().mean()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loss_epoch += loss.item()\n",
    "    print(f'Epcoh {e}: {loss_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Your Simple NN [10 pts]\n",
    "Now, it is time for you to implement your own model for this classification task. Your job here is to:\n",
    "1. Complete the SimpleNN class. It should be a 2- or 3-layer NN with proper non-linearity.\n",
    "2. Train your model with SGD optimizer.\n",
    "3. Tune your model a bit so you can achieve at least 80% accuracy on training set.\n",
    "Hint: you might want to look up `nn.ReLU`, `nn.Sigmoid`, `nn.BCELoss` in the [official document](https://pytorch.org/docs/stable/). You are allowed to freely pick the hyperparameters of your model.\n",
    "4. **Please note this is a binary classification problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Construct your small feedforward NN here.                                    #\n",
    "        ################################################################################\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # feed the input to your network, and output the predictions.                  #\n",
    "        ################################################################################\n",
    "        \n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10 # an epoch means looping through all the data in the datasets\n",
    "lr = 1e-1\n",
    "\n",
    "# create a simple model that is probably not gonna work well\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Initialize your model and SGD optimizer here.                                #\n",
    "################################################################################\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    loss_epoch = 0  # record accmulative loss for each epoch\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Loop through the dataloader and train your model with nn.BCELoss.            #\n",
    "    ################################################################################\n",
    "\n",
    "    ################################################################################\n",
    "    #                                 END OF YOUR CODE                             #\n",
    "    ################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for computing accuracy\n",
    "def get_acc(pred, y):\n",
    "    pred = pred.float()\n",
    "    y = y.float()\n",
    "    return (y==pred).sum().float()/y.size(0)*100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (model(X_train) > 0.5)\n",
    "train_acc = get_acc(y_pred, y_train)\n",
    "\n",
    "y_pred = (model(X_test) > 0.5)\n",
    "test_acc = get_acc(y_pred, y_test)\n",
    "print(f'Training accuracy: {train_acc}, Testing accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Image Classification with CNN [70 pts]\n",
    "Now, we are back to the image classification problem. In this section, our goal is to, again, train models on CIFAR-10 to perform image classification. Your tasks here are to:\n",
    "1. Build and Train a simple feed-forward Neural Network (consists of only nn.Linear layer with activation function) for the classification task\n",
    "2. Build and Train a **Convolutional** Neural Network (CNN) for the classification task\n",
    "3. Try different settings for training your CNN\n",
    "4. Reproduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we provide the code for creating a CIFAR10 dataloader. As you can see, PyTorch's `torchvision` package actually has an interface for the CIFAR10 dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Preprocessing steps on the training/testing data. You can define your own data augmentation\n",
    "# here, and PyTorch's API will do the rest for you.\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# This will automatically download the dataset for you if it cannot find the data in root\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Simple NN [10 pts]\n",
    "Implement a simple feed-forward neural network, and train it on the CIFAR-10 training set. Here's some specific requirements:\n",
    "1. The network should only consists of `nn.Linear` layers and the activation functions of your choices (e.g. `nn.Tanh`, `nn.ReLU`, `nn.Sigmoid`, etc). \n",
    "2. Train your model with `torch.optim.SGD` with the hyperparameters you like the most. \n",
    "\n",
    "Note that the hyperparameters work in previous assignment might not work the same, as the implementations of layers could be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Design and training [8 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Construct your small feedforward NN here.                                    #\n",
    "        ################################################################################\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # note that: here, the data is of the shape (B, C, H, W)\n",
    "        # where B is the batch size, C is color channels, and H\n",
    "        # and W is height and width.\n",
    "        # To feed it into the linear layer, we need to reshape it\n",
    "        # with .view() function.\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1) # reshape the data from (B, C, H, W) to (B, C*H*W)\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Forward pass, output the prediction score.                                   #\n",
    "        ################################################################################\n",
    "        \n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "lr = 1e-2\n",
    "n_input = 3072\n",
    "n_classes = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Your training code here.                                                     #\n",
    "################################################################################\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate your model with the helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_acc(model, loader):\n",
    "    ys = []\n",
    "    y_preds = []\n",
    "    for x, y in loader:\n",
    "        ys.append(y)\n",
    "        # set the prediction to the one that has highest value\n",
    "        # Note that the the output size of model(x) is (B, 10)\n",
    "        y_preds.append(torch.argmax(model(x), dim=1))\n",
    "    y = torch.cat(ys, dim=0)\n",
    "    y_pred = torch.cat(y_preds, dim=0)\n",
    "    print((y == y_pred).sum())\n",
    "    return get_acc(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Evaluate NN [2 pts]\n",
    "Evaluate your NN. You should get an accuracy around **50%** on training set and **49%** on testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = get_model_acc(model, train_loader)\n",
    "test_acc = get_model_acc(model, test_loader)\n",
    "print(f'Training accuracy: {train_acc}, Testing accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Convolutional Neural Network (CNN) [60 pts]\n",
    "Convolutional layer has been proven to be extremely useful for vision-based task. As mentioned in the lecture, this speical layer allows the model to learn filters that capture crucial visual features. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Implement and Evaluate CNN [15 pts]\n",
    "In this section, you will need to construct a CNN for classifying CIFAR-10 image. Specifically, you need to:\n",
    "1. build a `CNNClassifier` with `nn.Conv2d`, `nn.Maxpool2d` and activation functions that you think are appropriate. \n",
    "2. You would need to flatten the output of your convolutional networks with `view()`, and feed it into a `nn.Linear` layer to predict the class labels of the input. \n",
    "\n",
    "Once you are done with your module, train it with `optim.SGD`, and evaluate it. You should get an accuracy around **55%** on training set and **53%** on testing set.\n",
    "\n",
    "Hint: You might want to look up `nn.Conv2d`, `nn.Maxpool2d`, `nn.CrossEntropyLoss()`, `view()` and `size()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Construct a CNN with 2 or 3 convolutional layers and 1 linear layer for      #\n",
    "        # outputing class prediction. You are free to pick the hyperparameters         #\n",
    "        ################################################################################\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Forward pass of your network. First extract feature with CNN, and predict    #\n",
    "        # class scores with linear layer. Be careful about your input/output shape.    #\n",
    "        ################################################################################\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can tune these hyperparameters as you like.\n",
    "epoch = 10\n",
    "lr = 1e-1\n",
    "n_input = 3072\n",
    "n_classes = 10\n",
    "batch_size = 64\n",
    "num_workers = num_workers\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Your training code here.                                                     #\n",
    "################################################################################\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn on evaluation mode. This is crucial when you have BatchNorm in your network,\n",
    "# as you want to use the running mean/std you obtain durining training time to normalize\n",
    "# your input data. Rememeber to call .train() function after evaluation\n",
    "model.eval()\n",
    "train_acc = get_model_acc(model, train_loader)\n",
    "test_acc = get_model_acc(model, test_loader)\n",
    "print(f'Training accuracy: {train_acc}, Testing accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Explain your design and hyperparameter choice in three or four sentences:**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 STACK MORE LAYERS [20 pts]\n",
    "Now, **try at least 4 network architectures with different numbers of convolutional layers**. Train these settings with `optim.SGD`, plot the training/testing accuracy as a fuction of convolutional layers and describe what you have observed (running time, performance, etc). **Please make sure your figures are with clear legends and labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Your training code here.                                                     #\n",
    "################################################################################\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Briefly explain what you have observed in three or four sentences. Does stacking layers always give you better results? How about the computational time?:**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Optimizer? Optimizer! [15 pts]\n",
    "So far, we only use SGD as our optimizer. Now, pick two other optimizers, train your favorite CNN models, and compare the performance you get. What did you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Your training code here.                                                     #\n",
    "################################################################################\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**What did you see? Which optimizer is your favorite? Describe:**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Improve Your Model [10 pts]\n",
    "Again, we want you to play with your model a bit harder, and improve it. You are free to use everything you can find in the documents (`BatchNorm`, `SeLU`, etc), as long as it is not a **predefined network architectures in PyTorch package**. You can also implement some famous network architectures to push the performance. \n",
    "\n",
    "(A simple network with 5-6 `nn.Conv2d` can give you at least 70% accuracy on testing set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Your training code here.                                                     #\n",
    "################################################################################\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines for Downloading PDF in Google Colab\n",
    "- Run below cells only in Google Colab, Comment out in case of Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run below two lines (in google colab), installation steps to get .pdf of the notebook\n",
    "\n",
    "!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n",
    "!pip install pypandoc\n",
    "\n",
    "# After installation, comment above two lines and run again to remove installation comments from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find path to your notebook file in drive and enter in below line\n",
    "\n",
    "!jupyter nbconvert --to PDF \"your_notebook_path_here/DL_Assignment_2.ipynb\"\n",
    "\n",
    "#Example: \"/content/drive/My Drive/DL_Fall_2020/Assignment_2/DL_Assignment_2.ipynb\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
